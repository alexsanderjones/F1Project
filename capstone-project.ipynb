{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579455ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "from fastf1 import api\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from dfply import *\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#note: you should put in a new folder to store cached data, github doesn't push empty folders so you have to do it manually\n",
    "#In the zip, FastF1Cache should have data in it so it loads faster\n",
    "fastf1.Cache.enable_cache('FastF1Cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c0de9",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e5c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QualiScrape(grandPrix, raceDate, qualiDate):\n",
    "    \"\"\"\n",
    "    Input: grandPrix, Grand Prix's Race Date, Date of Qualifying\n",
    "    Returns: data frame with each driver matched to best qualifying time, along with tire and weather conditions\n",
    "    \"\"\"\n",
    "    #Build path to scrape data from\n",
    "    Quali = fastf1.api.make_path(grandPrix, raceDate, 'Qualifying', qualiDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    QualiTiming = fastf1.api.timing_data(Quali)[0]\n",
    "    QualiTimingApp = fastf1.api.timing_app_data(Quali)\n",
    "    QualiWeatherDict = fastf1.api.weather_data(Quali)\n",
    "    QualiWeatherData = pd.DataFrame.from_dict(QualiWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    QualiTiming = QualiTiming[QualiTiming.LapTime.notnull()]\n",
    "    QualiTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    QualiTimingApp = QualiTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(QualiTimingApp)):\n",
    "        if not QualiTimingApp.loc[index,\"Compound\"]:\n",
    "            QualiTimingApp.loc[index, \"Compound\"] = QualiTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not QualiTimingApp.loc[index,\"New\"]:\n",
    "            QualiTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if QualiTimingApp.loc[index,\"Time\"] == QualiTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not QualiTimingApp.loc[index,\"LapTime\"]:\n",
    "                QualiTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                QualiTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    QualiTimingApp = QualiTimingApp[QualiTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows\n",
    "    QualiTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    QualiTiming = QualiTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "\n",
    "    QualiCompiledTiming = pd.merge(QualiTiming,QualiTimingApp, on=['Time', 'Driver'])\n",
    "\n",
    "    QualiWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    QualiCompiledTiming = pd.merge(QualiCompiledTiming, QualiWeatherData, on=['Time'])\n",
    "    QualiCompiledTiming >>= arrange(X.Driver, X.Time)\n",
    "    QualiCompiledTiming = QualiCompiledTiming.reset_index()\n",
    "    \n",
    "    #Creating List of all drivers, then finding the best lap time for them\n",
    "    driverList = QualiCompiledTiming.Driver.unique()\n",
    "    QualiTimes = {}\n",
    "    for drv in driverList:\n",
    "        QualiTimes[drv] = [drv, QualiTiming[QualiTiming.Driver == drv].LapTime.min()]\n",
    "    QualiDF = pd.DataFrame.from_dict(QualiTimes, orient = 'index', columns = ['Driver','LapTime'])\n",
    "    QualiDF = QualiDF.reset_index()\n",
    "    QualiDF >>= select(X.Driver, X.LapTime)\n",
    "    \n",
    "    #Merge weather and tire data with appropriate laptime data\n",
    "    QualiDF = pd.merge(QualiDF, QualiCompiledTiming, on = [\"LapTime\", \"Driver\"])\n",
    "    \n",
    "    #Removing variables regarded as unnecessary, adding Qualifying prefix to Data so merging stays clean\n",
    "    #Note: Qualifying and Practice are merged by driver, so QualifyingDriver is renamed back to driver\n",
    "    QualiDF >>= select(X.Driver, X.LapTime, X.Compound, X.AirTemp, X.Humidity, X.Pressure, X.Rainfall, X.TrackTemp, X.WindSpeed) \\\n",
    "            >> mutate(LapTime = X.LapTime/ timedelta(seconds = 1))\n",
    "    QualiDF = QualiDF.add_prefix('Qualifying') >> rename(Driver = X.QualifyingDriver)\n",
    "    \n",
    "    return QualiDF\n",
    "\n",
    "def practiceScrape(grandPrix, raceDate, pracNum, pracDate):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix, Grand Prix Date, Which Practice, Which Date\n",
    "    Output: Dataframe with each driver's timed lap matched with tire compound and weather conditions\n",
    "    \"\"\"\n",
    "    #Make Path to scrape practice data\n",
    "    FP = fastf1.api.make_path(grandPrix, raceDate, f'Practice {pracNum}', pracDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    FPTiming = fastf1.api.timing_data(FP)[0]\n",
    "    FPTimingApp = fastf1.api.timing_app_data(FP)\n",
    "    FPWeatherDict = fastf1.api.weather_data(FP)\n",
    "    FPWeatherData = pd.DataFrame.from_dict(FPWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    FPTiming = FPTiming[FPTiming.LapTime.notnull()]\n",
    "    FPTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    FPTimingApp = FPTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(FPTimingApp)):\n",
    "        if not FPTimingApp.loc[index,\"Compound\"]:\n",
    "            FPTimingApp.loc[index, \"Compound\"] = FPTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not FPTimingApp.loc[index,\"New\"]:\n",
    "            FPTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if FPTimingApp.loc[index,\"Time\"] == FPTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not FPTimingApp.loc[index,\"LapTime\"]:\n",
    "                FPTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                FPTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    FPTimingApp = FPTimingApp[FPTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows, and rounding FPtiming session time to nearest minute\n",
    "    FPTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    FPTiming = FPTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "    \n",
    "    #Merge Dataframes, keyed by time and driver, then dropping irrelevant columns\n",
    "    FPCompiledTiming = pd.merge(FPTiming,FPTimingApp, on=['Time', 'Driver'])\n",
    "    FPWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    FPCompiledTiming = pd.merge(FPCompiledTiming, FPWeatherData, on=['Time'])\n",
    "    \n",
    "    #Final Cleaning, converting times into numerics\n",
    "    FPCompiledTiming >>= arrange(X.Driver, X.Time) >> mutate(Practice = pracNum) >> drop(X.WindDirection, X.Stint, X.TotalLaps)\\\n",
    "                     >> mutate(LapTime = X.LapTime/timedelta(seconds = 1)) >> mutate(Sector1Time = X.Sector1Time/timedelta(seconds = 1)) \\\n",
    "                     >> mutate(Sector2Time = X.Sector2Time/timedelta(seconds = 1)) >> mutate(Sector3Time = X.Sector3Time/timedelta(seconds = 1))\n",
    "    \n",
    "    fastestTime = FPCompiledTiming['LapTime'].min()\n",
    "    FPCompiledTiming = FPCompiledTiming.reset_index()\n",
    "    for index in range(len(FPCompiledTiming)):\n",
    "        if FPCompiledTiming.loc[index, 'LapTime'] > 1.14 * fastestTime:\n",
    "            FPCompiledTiming.loc[index, 'LapTime'] = np.nan      \n",
    "    FPCompiledTiming = FPCompiledTiming[FPCompiledTiming.LapTime.notnull()]\n",
    "    \n",
    "    \n",
    "    return FPCompiledTiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5b9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekendCompiler(grandPrix, raceDate, sprint = False):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix with appropriate race date, and whether grand prix was a sprint weekend\n",
    "    Output: Dataframe with all timed practice laps, merged with info about best qualifying lap\n",
    "    \"\"\"\n",
    "    #Creating relevant dates for session calling purposes\n",
    "    raceDate = datetime.strptime(raceDate, \"%Y-%m-%d\")\n",
    "    friDate = str(raceDate - timedelta(days = 2))[:10]\n",
    "    satDate = str(raceDate - timedelta(days = 1))[:10]\n",
    "    raceDate = str(raceDate)[:10]\n",
    "    \n",
    "    #Collecting Practice Data\n",
    "    #Note: On sprint weekends only one practice is done before qualifying, so FP1 is the only relevant data\n",
    "    FP1CompiledTiming = practiceScrape(grandPrix, raceDate, 1, friDate)\n",
    "    if not sprint:\n",
    "        FP2CompiledTiming = practiceScrape(grandPrix, raceDate, 2, friDate)\n",
    "        FP3CompiledTiming = practiceScrape(grandPrix, raceDate, 3, satDate)\n",
    "    \n",
    "    #Collecting Quali Data:\n",
    "    #Note: On sprint weekends, qualifying is done on Friday, as opposed to usual Saturdays\n",
    "    if not sprint:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, satDate)\n",
    "    else:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, friDate)\n",
    "    \n",
    "    #Concatenating Practice Dataframes\n",
    "    if not sprint:\n",
    "        frames = [FP1CompiledTiming, FP2CompiledTiming, FP3CompiledTiming]\n",
    "        practiceCompiled = pd.concat(frames)\n",
    "    else:\n",
    "        practiceCompiled = FP1CompiledTiming\n",
    "    \n",
    "    #Merging Quali Data with compiled Practice data, such that each laptime has the appropriate quali data\n",
    "    practiceCompiled = practiceCompiled.reset_index()\n",
    "    practiceCompiled >>= drop(X.level_0)\n",
    "    practiceCompiled = pd.merge(practiceCompiled, QualiDF, on=['Driver']) >> mutate(Weekend = grandPrix)\n",
    "    \n",
    "    return practiceCompiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34b074",
   "metadata": {},
   "source": [
    "Main Code for constructing data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73de0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Bahrain Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Saudi Arabian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Australian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Emilia Romagna Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Miami Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Spanish Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Monaco Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Azerbaijan Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Canadian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for British Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Austrian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for French Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Hungarian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Belgian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Dutch Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Italian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Singapore Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Japanese Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for United States Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Mexico City Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for São Paulo Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    }
   ],
   "source": [
    "raceCalendar = {1:[\"Bahrain Grand Prix\", \"2022-03-20\"], 2:[\"Saudi Arabian Grand Prix\", \"2022-03-27\"], 3:[\"Australian Grand Prix\", \"2022-04-10\"],\\\n",
    "                4:[\"Emilia Romagna Grand Prix\", \"2022-04-24\"], 5:[\"Miami Grand Prix\", \"2022-05-08\"], 6:[\"Spanish Grand Prix\", \"2022-05-22\"], \\\n",
    "                7:[\"Monaco Grand Prix\", \"2022-05-29\"], 8:[\"Azerbaijan Grand Prix\", \"2022-06-12\"], 9:[\"Canadian Grand Prix\",  \"2022-06-19\"], \\\n",
    "                10:[\"British Grand Prix\", \"2022-07-03\"], 11:[\"Austrian Grand Prix\", \"2022-07-10\"], 12:[\"French Grand Prix\", \"2022-07-24\"], \\\n",
    "                13:[\"Hungarian Grand Prix\", \"2022-07-31\"], 14:[\"Belgian Grand Prix\", \"2022-08-28\"], 15:[\"Dutch Grand Prix\", \"2022-09-04\"], \\\n",
    "                16:[\"Italian Grand Prix\", \"2022-09-11\"], 17:[\"Singapore Grand Prix\", \"2022-10-02\"], 18:[\"Japanese Grand Prix\", \"2022-10-09\"], \\\n",
    "                19:[\"United States Grand Prix\", \"2022-10-23\"], 20:[\"Mexico City Grand Prix\", \"2022-10-30\"], 21:[\"São Paulo Grand Prix\", \"2022-11-13\"]}\n",
    "dfList = []\n",
    "for key in raceCalendar:\n",
    "    \n",
    "    print(f\"Collecting Data for {raceCalendar[key][0]}\")\n",
    "    #Imola, Austria, and Brazil are sprint weekends, so they have a special case where sprint = true\n",
    "    if key not in [4,11,21]:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1]))\n",
    "    else:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1], sprint = True))\n",
    "\n",
    "masterList = pd.concat(dfList)\n",
    "masterList >>= drop(X.Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c937f",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb0571",
   "metadata": {},
   "source": [
    "Clean Data For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63740b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn_copy = masterList.copy()\n",
    "nn_copy = pd.get_dummies(nn_copy, columns = ['Weekend', 'Driver', 'Compound', 'QualifyingCompound'])\n",
    "\n",
    "#Adjust Boolean Data\n",
    "nn_copy['New'] = pd.to_numeric(nn_copy['New'], errors = 'coerce')\n",
    "\n",
    "nn_copy = nn_copy.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52f51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training variables(X) and prediction variables (Y)\n",
    "x = nn_copy >> drop(X.QualifyingLapTime)\n",
    "             \n",
    "y = nn_copy['QualifyingLapTime']\n",
    "\n",
    "#Normalize Training Data\n",
    "\n",
    "x_scaler = preprocessing.StandardScaler().fit(x)\n",
    "x_norm = x_scaler.transform(x)\n",
    "\n",
    "#Split Data Into Test and Training Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size = 0.2)\n",
    "\n",
    "x_train = np.asarray(x_train).astype(float)\n",
    "y_train = np.asarray(y_train).astype(float)\n",
    "x_test = np.asarray(x_test).astype(float)\n",
    "y_test = np.asarray(y_test).astype(float)\n",
    "\n",
    "#Split Test Data Into Validation and Test Sets\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c1bca",
   "metadata": {},
   "source": [
    "\"Standard\" Neural Network Following General Guidelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3188566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "225/225 - 1s - loss: 7564.7422 - mean_squared_error: 7564.7422 - mean_absolute_error: 86.2059 - 1s/epoch - 6ms/step\n",
      "Epoch 2/100\n",
      "225/225 - 0s - loss: 7389.1919 - mean_squared_error: 7389.1919 - mean_absolute_error: 85.2032 - 375ms/epoch - 2ms/step\n",
      "Epoch 3/100\n",
      "225/225 - 0s - loss: 7199.6069 - mean_squared_error: 7199.6069 - mean_absolute_error: 84.0845 - 395ms/epoch - 2ms/step\n",
      "Epoch 4/100\n",
      "225/225 - 0s - loss: 7011.0410 - mean_squared_error: 7011.0410 - mean_absolute_error: 82.9385 - 417ms/epoch - 2ms/step\n",
      "Epoch 5/100\n",
      "225/225 - 0s - loss: 6828.6509 - mean_squared_error: 6828.6509 - mean_absolute_error: 81.7932 - 452ms/epoch - 2ms/step\n",
      "Epoch 6/100\n",
      "225/225 - 0s - loss: 6653.1006 - mean_squared_error: 6653.1006 - mean_absolute_error: 80.6567 - 439ms/epoch - 2ms/step\n",
      "Epoch 7/100\n",
      "225/225 - 0s - loss: 6484.8652 - mean_squared_error: 6484.8652 - mean_absolute_error: 79.5293 - 407ms/epoch - 2ms/step\n",
      "Epoch 8/100\n",
      "225/225 - 0s - loss: 6324.3911 - mean_squared_error: 6324.3911 - mean_absolute_error: 78.4175 - 438ms/epoch - 2ms/step\n",
      "Epoch 9/100\n",
      "225/225 - 0s - loss: 6170.7070 - mean_squared_error: 6170.7070 - mean_absolute_error: 77.3181 - 449ms/epoch - 2ms/step\n",
      "Epoch 10/100\n",
      "225/225 - 0s - loss: 6023.7007 - mean_squared_error: 6023.7007 - mean_absolute_error: 76.2308 - 409ms/epoch - 2ms/step\n",
      "Epoch 11/100\n",
      "225/225 - 0s - loss: 5882.5444 - mean_squared_error: 5882.5444 - mean_absolute_error: 75.1550 - 419ms/epoch - 2ms/step\n",
      "Epoch 12/100\n",
      "225/225 - 0s - loss: 5747.5332 - mean_squared_error: 5747.5332 - mean_absolute_error: 74.0907 - 381ms/epoch - 2ms/step\n",
      "Epoch 13/100\n",
      "225/225 - 0s - loss: 5618.7861 - mean_squared_error: 5618.7861 - mean_absolute_error: 73.0424 - 433ms/epoch - 2ms/step\n",
      "Epoch 14/100\n",
      "225/225 - 0s - loss: 5496.0273 - mean_squared_error: 5496.0273 - mean_absolute_error: 72.0068 - 381ms/epoch - 2ms/step\n",
      "Epoch 15/100\n",
      "225/225 - 0s - loss: 5379.0664 - mean_squared_error: 5379.0664 - mean_absolute_error: 70.9882 - 430ms/epoch - 2ms/step\n",
      "Epoch 16/100\n",
      "225/225 - 0s - loss: 5267.5659 - mean_squared_error: 5267.5659 - mean_absolute_error: 69.9824 - 398ms/epoch - 2ms/step\n",
      "Epoch 17/100\n",
      "225/225 - 0s - loss: 5160.9697 - mean_squared_error: 5160.9697 - mean_absolute_error: 68.9916 - 418ms/epoch - 2ms/step\n",
      "Epoch 18/100\n",
      "225/225 - 0s - loss: 5059.3584 - mean_squared_error: 5059.3584 - mean_absolute_error: 68.0167 - 397ms/epoch - 2ms/step\n",
      "Epoch 19/100\n",
      "225/225 - 0s - loss: 4962.4482 - mean_squared_error: 4962.4482 - mean_absolute_error: 67.0658 - 425ms/epoch - 2ms/step\n",
      "Epoch 20/100\n",
      "225/225 - 0s - loss: 4870.3301 - mean_squared_error: 4870.3301 - mean_absolute_error: 66.1369 - 444ms/epoch - 2ms/step\n",
      "Epoch 21/100\n",
      "225/225 - 0s - loss: 4782.6235 - mean_squared_error: 4782.6235 - mean_absolute_error: 65.2316 - 412ms/epoch - 2ms/step\n",
      "Epoch 22/100\n",
      "225/225 - 0s - loss: 4699.2134 - mean_squared_error: 4699.2134 - mean_absolute_error: 64.3443 - 409ms/epoch - 2ms/step\n",
      "Epoch 23/100\n",
      "225/225 - 0s - loss: 4619.8442 - mean_squared_error: 4619.8442 - mean_absolute_error: 63.4727 - 437ms/epoch - 2ms/step\n",
      "Epoch 24/100\n",
      "225/225 - 0s - loss: 4544.4512 - mean_squared_error: 4544.4512 - mean_absolute_error: 62.6160 - 358ms/epoch - 2ms/step\n",
      "Epoch 25/100\n",
      "225/225 - 0s - loss: 4472.7065 - mean_squared_error: 4472.7065 - mean_absolute_error: 61.7774 - 433ms/epoch - 2ms/step\n",
      "Epoch 26/100\n",
      "225/225 - 0s - loss: 4404.2886 - mean_squared_error: 4404.2886 - mean_absolute_error: 60.9492 - 433ms/epoch - 2ms/step\n",
      "Epoch 27/100\n",
      "225/225 - 0s - loss: 4339.0420 - mean_squared_error: 4339.0420 - mean_absolute_error: 60.1371 - 398ms/epoch - 2ms/step\n",
      "Epoch 28/100\n",
      "225/225 - 0s - loss: 4276.8545 - mean_squared_error: 4276.8545 - mean_absolute_error: 59.3416 - 413ms/epoch - 2ms/step\n",
      "Epoch 29/100\n",
      "225/225 - 0s - loss: 4217.7896 - mean_squared_error: 4217.7896 - mean_absolute_error: 58.5638 - 393ms/epoch - 2ms/step\n",
      "Epoch 30/100\n",
      "225/225 - 0s - loss: 4161.8516 - mean_squared_error: 4161.8516 - mean_absolute_error: 57.8077 - 390ms/epoch - 2ms/step\n",
      "Epoch 31/100\n",
      "225/225 - 0s - loss: 4108.8691 - mean_squared_error: 4108.8691 - mean_absolute_error: 57.0761 - 422ms/epoch - 2ms/step\n",
      "Epoch 32/100\n",
      "225/225 - 0s - loss: 4058.5425 - mean_squared_error: 4058.5425 - mean_absolute_error: 56.3672 - 416ms/epoch - 2ms/step\n",
      "Epoch 33/100\n",
      "225/225 - 0s - loss: 4010.8086 - mean_squared_error: 4010.8086 - mean_absolute_error: 55.6816 - 408ms/epoch - 2ms/step\n",
      "Epoch 34/100\n",
      "225/225 - 0s - loss: 3965.3909 - mean_squared_error: 3965.3909 - mean_absolute_error: 55.0185 - 402ms/epoch - 2ms/step\n",
      "Epoch 35/100\n",
      "225/225 - 0s - loss: 3922.1675 - mean_squared_error: 3922.1675 - mean_absolute_error: 54.3763 - 379ms/epoch - 2ms/step\n",
      "Epoch 36/100\n",
      "225/225 - 0s - loss: 3881.0481 - mean_squared_error: 3881.0481 - mean_absolute_error: 53.7564 - 418ms/epoch - 2ms/step\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMSE, metrics \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError(),\n\u001b[0;32m     10\u001b[0m                                                                          tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanAbsoluteError()])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Start Batch Size Small (Powers of 2 for efficient GPU usage)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Epochs rule of thumb is to start with triple the amount of columns\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Verbose determines how it prints the training progress\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Evaluate Model \u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(x_validation, y_validation, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#One layer is generally enough for simple problems like this (As opposed to something like digit recognition)\n",
    "#Hidden layer size is generally between the input size (number of x variables) and the output size (number of y variables)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "\n",
    "#Adam is the recommended default optimizer\n",
    "#Mean squared error loss function as seen in class\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "#Start Batch Size Small (Powers of 2 for efficient GPU usage)\n",
    "#Epochs rule of thumb is to start with triple the amount of columns\n",
    "#Verbose determines how it prints the training progress\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)\n",
    "\n",
    "#Evaluate Model \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd464f54",
   "metadata": {},
   "source": [
    "What If We Adjust The Network Architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f33a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e230a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is best\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate best model with test data\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7496c07",
   "metadata": {},
   "source": [
    "What if we change the activation function? (They are all worse than relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'linear'),\n",
    "    tf.keras.layers.Dense(16, activation = 'linear'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99571dcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(16, activation = 'sigmoid'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(16, activation = 'tanh'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7dcbf",
   "metadata": {},
   "source": [
    "## What if we adjust batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b8a5f",
   "metadata": {},
   "source": [
    "Smaller is generally better, but we found batch size = 32 the best regardless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74418d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 64, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d007e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is better\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 8, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is better\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 4, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52939d",
   "metadata": {},
   "source": [
    "What if we add more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183d464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "225/225 - 1s - loss: 6801.3164 - mean_squared_error: 6801.3164 - mean_absolute_error: 81.5795 - 1s/epoch - 5ms/step\n",
      "Epoch 2/250\n",
      "225/225 - 0s - loss: 2872.1313 - mean_squared_error: 2872.1313 - mean_absolute_error: 51.1160 - 423ms/epoch - 2ms/step\n",
      "Epoch 3/250\n",
      "225/225 - 0s - loss: 437.3517 - mean_squared_error: 437.3517 - mean_absolute_error: 17.3478 - 420ms/epoch - 2ms/step\n",
      "Epoch 4/250\n",
      "225/225 - 0s - loss: 99.4488 - mean_squared_error: 99.4488 - mean_absolute_error: 7.6412 - 439ms/epoch - 2ms/step\n",
      "Epoch 5/250\n",
      "225/225 - 0s - loss: 55.7046 - mean_squared_error: 55.7046 - mean_absolute_error: 5.6029 - 421ms/epoch - 2ms/step\n",
      "Epoch 6/250\n",
      "225/225 - 0s - loss: 40.9744 - mean_squared_error: 40.9744 - mean_absolute_error: 4.7474 - 432ms/epoch - 2ms/step\n",
      "Epoch 7/250\n",
      "225/225 - 0s - loss: 32.8623 - mean_squared_error: 32.8623 - mean_absolute_error: 4.2237 - 416ms/epoch - 2ms/step\n",
      "Epoch 8/250\n",
      "225/225 - 0s - loss: 27.3554 - mean_squared_error: 27.3554 - mean_absolute_error: 3.8356 - 394ms/epoch - 2ms/step\n",
      "Epoch 9/250\n",
      "225/225 - 0s - loss: 23.2692 - mean_squared_error: 23.2692 - mean_absolute_error: 3.5195 - 452ms/epoch - 2ms/step\n",
      "Epoch 10/250\n",
      "225/225 - 0s - loss: 20.0736 - mean_squared_error: 20.0736 - mean_absolute_error: 3.2545 - 442ms/epoch - 2ms/step\n",
      "Epoch 11/250\n",
      "225/225 - 0s - loss: 17.5802 - mean_squared_error: 17.5802 - mean_absolute_error: 3.0305 - 414ms/epoch - 2ms/step\n",
      "Epoch 12/250\n",
      "225/225 - 0s - loss: 15.4436 - mean_squared_error: 15.4436 - mean_absolute_error: 2.8313 - 405ms/epoch - 2ms/step\n",
      "Epoch 13/250\n",
      "225/225 - 0s - loss: 13.6759 - mean_squared_error: 13.6759 - mean_absolute_error: 2.6503 - 453ms/epoch - 2ms/step\n",
      "Epoch 14/250\n",
      "225/225 - 0s - loss: 12.2213 - mean_squared_error: 12.2213 - mean_absolute_error: 2.4974 - 412ms/epoch - 2ms/step\n",
      "Epoch 15/250\n",
      "225/225 - 0s - loss: 10.9160 - mean_squared_error: 10.9160 - mean_absolute_error: 2.3467 - 435ms/epoch - 2ms/step\n",
      "Epoch 16/250\n",
      "225/225 - 0s - loss: 9.8885 - mean_squared_error: 9.8885 - mean_absolute_error: 2.2203 - 434ms/epoch - 2ms/step\n",
      "Epoch 17/250\n",
      "225/225 - 0s - loss: 8.9280 - mean_squared_error: 8.9280 - mean_absolute_error: 2.1067 - 397ms/epoch - 2ms/step\n",
      "Epoch 18/250\n",
      "225/225 - 0s - loss: 8.1096 - mean_squared_error: 8.1096 - mean_absolute_error: 2.0041 - 369ms/epoch - 2ms/step\n",
      "Epoch 19/250\n",
      "225/225 - 0s - loss: 7.4142 - mean_squared_error: 7.4142 - mean_absolute_error: 1.8999 - 392ms/epoch - 2ms/step\n",
      "Epoch 20/250\n",
      "225/225 - 0s - loss: 6.7844 - mean_squared_error: 6.7844 - mean_absolute_error: 1.8072 - 444ms/epoch - 2ms/step\n",
      "Epoch 21/250\n",
      "225/225 - 0s - loss: 6.2339 - mean_squared_error: 6.2339 - mean_absolute_error: 1.7265 - 455ms/epoch - 2ms/step\n",
      "Epoch 22/250\n",
      "225/225 - 0s - loss: 5.7649 - mean_squared_error: 5.7649 - mean_absolute_error: 1.6440 - 432ms/epoch - 2ms/step\n",
      "Epoch 23/250\n",
      "225/225 - 0s - loss: 5.3409 - mean_squared_error: 5.3409 - mean_absolute_error: 1.5748 - 437ms/epoch - 2ms/step\n",
      "Epoch 24/250\n",
      "225/225 - 0s - loss: 4.9836 - mean_squared_error: 4.9836 - mean_absolute_error: 1.5085 - 368ms/epoch - 2ms/step\n",
      "Epoch 25/250\n",
      "225/225 - 0s - loss: 4.6148 - mean_squared_error: 4.6148 - mean_absolute_error: 1.4439 - 426ms/epoch - 2ms/step\n",
      "Epoch 26/250\n",
      "225/225 - 0s - loss: 4.3161 - mean_squared_error: 4.3161 - mean_absolute_error: 1.3855 - 452ms/epoch - 2ms/step\n",
      "Epoch 27/250\n",
      "225/225 - 0s - loss: 4.0371 - mean_squared_error: 4.0371 - mean_absolute_error: 1.3312 - 420ms/epoch - 2ms/step\n",
      "Epoch 28/250\n",
      "225/225 - 0s - loss: 3.7763 - mean_squared_error: 3.7763 - mean_absolute_error: 1.2809 - 456ms/epoch - 2ms/step\n",
      "Epoch 29/250\n",
      "225/225 - 0s - loss: 3.5616 - mean_squared_error: 3.5616 - mean_absolute_error: 1.2348 - 440ms/epoch - 2ms/step\n",
      "Epoch 30/250\n",
      "225/225 - 0s - loss: 3.3455 - mean_squared_error: 3.3455 - mean_absolute_error: 1.1852 - 417ms/epoch - 2ms/step\n",
      "Epoch 31/250\n",
      "225/225 - 0s - loss: 3.1393 - mean_squared_error: 3.1393 - mean_absolute_error: 1.1431 - 471ms/epoch - 2ms/step\n",
      "Epoch 32/250\n",
      "225/225 - 0s - loss: 2.9831 - mean_squared_error: 2.9831 - mean_absolute_error: 1.1133 - 431ms/epoch - 2ms/step\n",
      "Epoch 33/250\n",
      "225/225 - 0s - loss: 2.8004 - mean_squared_error: 2.8004 - mean_absolute_error: 1.0691 - 439ms/epoch - 2ms/step\n",
      "Epoch 34/250\n",
      "225/225 - 0s - loss: 2.6530 - mean_squared_error: 2.6530 - mean_absolute_error: 1.0392 - 445ms/epoch - 2ms/step\n",
      "Epoch 35/250\n",
      "225/225 - 0s - loss: 2.5293 - mean_squared_error: 2.5293 - mean_absolute_error: 1.0039 - 404ms/epoch - 2ms/step\n",
      "Epoch 36/250\n",
      "225/225 - 0s - loss: 2.3921 - mean_squared_error: 2.3921 - mean_absolute_error: 0.9786 - 454ms/epoch - 2ms/step\n",
      "Epoch 37/250\n",
      "225/225 - 0s - loss: 2.2493 - mean_squared_error: 2.2493 - mean_absolute_error: 0.9382 - 445ms/epoch - 2ms/step\n",
      "Epoch 38/250\n",
      "225/225 - 0s - loss: 2.1711 - mean_squared_error: 2.1711 - mean_absolute_error: 0.9174 - 352ms/epoch - 2ms/step\n",
      "Epoch 39/250\n",
      "225/225 - 0s - loss: 2.0588 - mean_squared_error: 2.0588 - mean_absolute_error: 0.8868 - 435ms/epoch - 2ms/step\n",
      "Epoch 40/250\n",
      "225/225 - 0s - loss: 1.9604 - mean_squared_error: 1.9604 - mean_absolute_error: 0.8696 - 425ms/epoch - 2ms/step\n",
      "Epoch 41/250\n",
      "225/225 - 0s - loss: 1.8583 - mean_squared_error: 1.8583 - mean_absolute_error: 0.8304 - 368ms/epoch - 2ms/step\n",
      "Epoch 42/250\n",
      "225/225 - 0s - loss: 1.7957 - mean_squared_error: 1.7957 - mean_absolute_error: 0.8218 - 467ms/epoch - 2ms/step\n",
      "Epoch 43/250\n",
      "225/225 - 0s - loss: 1.7358 - mean_squared_error: 1.7358 - mean_absolute_error: 0.7959 - 433ms/epoch - 2ms/step\n",
      "Epoch 44/250\n",
      "225/225 - 0s - loss: 1.6527 - mean_squared_error: 1.6527 - mean_absolute_error: 0.7776 - 390ms/epoch - 2ms/step\n",
      "Epoch 45/250\n",
      "225/225 - 0s - loss: 1.5923 - mean_squared_error: 1.5923 - mean_absolute_error: 0.7585 - 394ms/epoch - 2ms/step\n",
      "Epoch 46/250\n",
      "225/225 - 0s - loss: 1.5249 - mean_squared_error: 1.5249 - mean_absolute_error: 0.7313 - 399ms/epoch - 2ms/step\n",
      "Epoch 47/250\n",
      "225/225 - 0s - loss: 1.4841 - mean_squared_error: 1.4841 - mean_absolute_error: 0.7346 - 457ms/epoch - 2ms/step\n",
      "Epoch 48/250\n",
      "225/225 - 0s - loss: 1.4262 - mean_squared_error: 1.4262 - mean_absolute_error: 0.7042 - 418ms/epoch - 2ms/step\n",
      "Epoch 49/250\n",
      "225/225 - 0s - loss: 1.3985 - mean_squared_error: 1.3985 - mean_absolute_error: 0.6996 - 394ms/epoch - 2ms/step\n",
      "Epoch 50/250\n",
      "225/225 - 0s - loss: 1.3496 - mean_squared_error: 1.3496 - mean_absolute_error: 0.6798 - 407ms/epoch - 2ms/step\n",
      "Epoch 51/250\n",
      "225/225 - 0s - loss: 1.3343 - mean_squared_error: 1.3343 - mean_absolute_error: 0.6751 - 432ms/epoch - 2ms/step\n",
      "Epoch 52/250\n",
      "225/225 - 0s - loss: 1.3026 - mean_squared_error: 1.3026 - mean_absolute_error: 0.6688 - 423ms/epoch - 2ms/step\n",
      "Epoch 53/250\n",
      "225/225 - 0s - loss: 1.2506 - mean_squared_error: 1.2506 - mean_absolute_error: 0.6452 - 425ms/epoch - 2ms/step\n",
      "Epoch 54/250\n",
      "225/225 - 0s - loss: 1.1977 - mean_squared_error: 1.1977 - mean_absolute_error: 0.6287 - 433ms/epoch - 2ms/step\n",
      "Epoch 55/250\n",
      "225/225 - 0s - loss: 1.1875 - mean_squared_error: 1.1875 - mean_absolute_error: 0.6264 - 425ms/epoch - 2ms/step\n",
      "Epoch 56/250\n",
      "225/225 - 0s - loss: 1.1414 - mean_squared_error: 1.1414 - mean_absolute_error: 0.6136 - 453ms/epoch - 2ms/step\n",
      "Epoch 57/250\n",
      "225/225 - 0s - loss: 1.1533 - mean_squared_error: 1.1533 - mean_absolute_error: 0.6055 - 464ms/epoch - 2ms/step\n",
      "Epoch 58/250\n",
      "225/225 - 0s - loss: 1.0885 - mean_squared_error: 1.0885 - mean_absolute_error: 0.5919 - 435ms/epoch - 2ms/step\n",
      "Epoch 59/250\n",
      "225/225 - 0s - loss: 1.0739 - mean_squared_error: 1.0739 - mean_absolute_error: 0.5791 - 425ms/epoch - 2ms/step\n",
      "Epoch 60/250\n",
      "225/225 - 0s - loss: 1.0241 - mean_squared_error: 1.0241 - mean_absolute_error: 0.5660 - 436ms/epoch - 2ms/step\n",
      "Epoch 61/250\n",
      "225/225 - 0s - loss: 1.0658 - mean_squared_error: 1.0658 - mean_absolute_error: 0.5787 - 459ms/epoch - 2ms/step\n",
      "Epoch 62/250\n",
      "225/225 - 0s - loss: 0.9792 - mean_squared_error: 0.9792 - mean_absolute_error: 0.5471 - 457ms/epoch - 2ms/step\n",
      "Epoch 63/250\n",
      "225/225 - 0s - loss: 0.9452 - mean_squared_error: 0.9452 - mean_absolute_error: 0.5393 - 437ms/epoch - 2ms/step\n",
      "Epoch 64/250\n",
      "225/225 - 0s - loss: 0.9317 - mean_squared_error: 0.9317 - mean_absolute_error: 0.5364 - 449ms/epoch - 2ms/step\n",
      "Epoch 65/250\n",
      "225/225 - 0s - loss: 0.9063 - mean_squared_error: 0.9063 - mean_absolute_error: 0.5286 - 467ms/epoch - 2ms/step\n",
      "Epoch 66/250\n",
      "225/225 - 0s - loss: 0.8967 - mean_squared_error: 0.8967 - mean_absolute_error: 0.5162 - 462ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/250\n",
      "225/225 - 0s - loss: 0.8564 - mean_squared_error: 0.8564 - mean_absolute_error: 0.5164 - 414ms/epoch - 2ms/step\n",
      "Epoch 68/250\n",
      "225/225 - 0s - loss: 0.8454 - mean_squared_error: 0.8454 - mean_absolute_error: 0.5086 - 479ms/epoch - 2ms/step\n",
      "Epoch 69/250\n",
      "225/225 - 0s - loss: 0.8183 - mean_squared_error: 0.8183 - mean_absolute_error: 0.4939 - 457ms/epoch - 2ms/step\n",
      "Epoch 70/250\n",
      "225/225 - 0s - loss: 0.7865 - mean_squared_error: 0.7865 - mean_absolute_error: 0.4841 - 449ms/epoch - 2ms/step\n",
      "Epoch 71/250\n",
      "225/225 - 0s - loss: 0.7663 - mean_squared_error: 0.7663 - mean_absolute_error: 0.4720 - 413ms/epoch - 2ms/step\n",
      "Epoch 72/250\n",
      "225/225 - 0s - loss: 0.7344 - mean_squared_error: 0.7344 - mean_absolute_error: 0.4646 - 421ms/epoch - 2ms/step\n",
      "Epoch 73/250\n",
      "225/225 - 0s - loss: 0.7179 - mean_squared_error: 0.7179 - mean_absolute_error: 0.4605 - 474ms/epoch - 2ms/step\n",
      "Epoch 74/250\n",
      "225/225 - 0s - loss: 0.7108 - mean_squared_error: 0.7108 - mean_absolute_error: 0.4581 - 468ms/epoch - 2ms/step\n",
      "Epoch 75/250\n",
      "225/225 - 0s - loss: 0.6824 - mean_squared_error: 0.6824 - mean_absolute_error: 0.4520 - 450ms/epoch - 2ms/step\n",
      "Epoch 76/250\n",
      "225/225 - 0s - loss: 0.6617 - mean_squared_error: 0.6617 - mean_absolute_error: 0.4373 - 436ms/epoch - 2ms/step\n",
      "Epoch 77/250\n",
      "225/225 - 0s - loss: 0.6538 - mean_squared_error: 0.6538 - mean_absolute_error: 0.4355 - 392ms/epoch - 2ms/step\n",
      "Epoch 78/250\n",
      "225/225 - 0s - loss: 0.6138 - mean_squared_error: 0.6138 - mean_absolute_error: 0.4256 - 408ms/epoch - 2ms/step\n",
      "Epoch 79/250\n",
      "225/225 - 0s - loss: 0.6155 - mean_squared_error: 0.6155 - mean_absolute_error: 0.4217 - 408ms/epoch - 2ms/step\n",
      "Epoch 80/250\n",
      "225/225 - 0s - loss: 0.5910 - mean_squared_error: 0.5910 - mean_absolute_error: 0.4169 - 452ms/epoch - 2ms/step\n",
      "Epoch 81/250\n",
      "225/225 - 0s - loss: 0.5715 - mean_squared_error: 0.5715 - mean_absolute_error: 0.4017 - 394ms/epoch - 2ms/step\n",
      "Epoch 82/250\n",
      "225/225 - 0s - loss: 0.5812 - mean_squared_error: 0.5812 - mean_absolute_error: 0.4213 - 403ms/epoch - 2ms/step\n",
      "Epoch 83/250\n",
      "225/225 - 0s - loss: 0.5616 - mean_squared_error: 0.5616 - mean_absolute_error: 0.4046 - 431ms/epoch - 2ms/step\n",
      "Epoch 84/250\n",
      "225/225 - 0s - loss: 0.5321 - mean_squared_error: 0.5321 - mean_absolute_error: 0.3905 - 454ms/epoch - 2ms/step\n",
      "Epoch 85/250\n",
      "225/225 - 0s - loss: 0.5189 - mean_squared_error: 0.5189 - mean_absolute_error: 0.3910 - 449ms/epoch - 2ms/step\n",
      "Epoch 86/250\n",
      "225/225 - 0s - loss: 0.5147 - mean_squared_error: 0.5147 - mean_absolute_error: 0.3874 - 445ms/epoch - 2ms/step\n",
      "Epoch 87/250\n",
      "225/225 - 0s - loss: 0.5077 - mean_squared_error: 0.5077 - mean_absolute_error: 0.3855 - 344ms/epoch - 2ms/step\n",
      "Epoch 88/250\n",
      "225/225 - 0s - loss: 0.4956 - mean_squared_error: 0.4956 - mean_absolute_error: 0.3864 - 447ms/epoch - 2ms/step\n",
      "Epoch 89/250\n",
      "225/225 - 0s - loss: 0.4730 - mean_squared_error: 0.4730 - mean_absolute_error: 0.3763 - 458ms/epoch - 2ms/step\n",
      "Epoch 90/250\n",
      "225/225 - 0s - loss: 0.4621 - mean_squared_error: 0.4621 - mean_absolute_error: 0.3730 - 460ms/epoch - 2ms/step\n",
      "Epoch 91/250\n"
     ]
    }
   ],
   "source": [
    "#Much better but takes a long time to train and could cause overfitting (Best model with all the data)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                   tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=250, verbose=2)\n",
    "model.evaluate(x_validation, y_validation, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce465a13",
   "metadata": {},
   "source": [
    "Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837079e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39386aa7",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genModel(layers, batchSize=32, epochs=100, verbose=2):\n",
    "    \"\"\"\n",
    "    Automated neural network generator\n",
    "    Input: list of layers, batch size, number of epochs, verbosity\n",
    "    Output: it do the thing\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential(layers)\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                   tf.keras.metrics.MeanAbsoluteError()])\n",
    "    model.fit(x_train, y_train, batchSize, epochs, verbose)\n",
    "    model.evaluate(x_validation, y_validation, verbose)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "class PlotData():\n",
    "    \"\"\"\n",
    "    A module for storing plot data in a more portable, reusable form\n",
    "    Stores x and y data and metadata (title, labels, etc.)\n",
    "    Parameters: 2 arrays of x and y datasets (to allow multiple relations per plot), \n",
    "                plot type as a string, title, axis labels, and grid boolean\n",
    "    \"\"\"\n",
    "    def __init__(self, xdata, ydata, ptype='plot', title='Figure', xlabel='x axis', ylabel='y axis', grid=True):\n",
    "        \n",
    "        # Only check these if we have ydata\n",
    "        if ydata != None:\n",
    "            # Throws error if xdata and ydata length do not match\n",
    "            if not len(xdata) == len(ydata):\n",
    "                raise Exception('Data mismatch error: must have the same number of corresponding x and y fields')\n",
    "            \n",
    "            # Throws error if x and y subdata length do not match\n",
    "                for i in range(len(xdata)):\n",
    "                    if not len(xdata[i]) == len(ydata[i]):\n",
    "                        raise Exception(f'Data mismatch error: {i}th set of x and y data have unequal length')\n",
    "        \n",
    "            \n",
    "        self.xdata = xdata\n",
    "        self.ydata = ydata\n",
    "        self.ptype = ptype\n",
    "        self.title = title\n",
    "        self.xlabel = xlabel\n",
    "        self.ylabel = ylabel\n",
    "        self.grid = grid\n",
    "        \n",
    "        \n",
    "    def plot(self, style='b--'):\n",
    "        \"\"\"\n",
    "        May not be showing it but it do be plotting the data\n",
    "        Input: Optional style\n",
    "        \"\"\"\n",
    "        # Plots based on type\n",
    "        if self.ptype == 'plot':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.plot(self.xdata[i], self.ydata[i], style)\n",
    "                \n",
    "        elif self.ptype == 'scatter':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.scatter(self.xdata[i], self.ydata[i], style)\n",
    "            \n",
    "        elif self.ptype == 'bar':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.bar(self.xdata[i], self.ydata[i])\n",
    "                \n",
    "        elif self.ptype == 'boxplot':\n",
    "            plt.boxplot(self.xdata)\n",
    "            \n",
    "        else:\n",
    "            print(f'{self.title} has invalid plot type')\n",
    "                \n",
    "        plt.title(self.title)\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if self.grid:\n",
    "            plt.grid()\n",
    "            \n",
    "            \n",
    "    def addData(x, y):\n",
    "        \"\"\"\n",
    "        Clears the figure and adds a new x-y relation to the data\n",
    "        Input: x and y data to add\n",
    "        Output: It adds the data wow\n",
    "        \"\"\"\n",
    "        plt.clf()\n",
    "        self.xdata.append(x)\n",
    "        self.ydata.append(y)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35125264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter():\n",
    "    \"\"\"\n",
    "    Takes plot data and auto-generates subplots\n",
    "    Parameters: a list of PlotData objects\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, plots, subdim=None):\n",
    "        self.plots = plots\n",
    "        self.subdim = subdim\n",
    "        self.arrange(self.subdim)\n",
    "        self.figsize = (5*self.subdim[1], 5*self.subdim[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def addPlot(plot, loc=-1, subdim=None):\n",
    "        \"\"\"\n",
    "        Adds a new subplot and rearranges the figure\n",
    "        Input: the PlotData to add, the location to display it, and a custom aspect ratio if desired\n",
    "        Output:  It does it\n",
    "        \"\"\"\n",
    "        self.plots.insert(loc, plot)\n",
    "        if not subdim == None:\n",
    "            self.subdim = subdim\n",
    "        else:\n",
    "            arrange()\n",
    "    \n",
    "    \n",
    "    def addData(x, y, loc):\n",
    "        \"\"\"\n",
    "        Adds data to a subplot\n",
    "        Input: x and y data to add, and which subplot to add to\n",
    "        Output: It also does it\n",
    "        \"\"\"\n",
    "        self.plots[loc].addData(x, y)\n",
    "    \n",
    "    \n",
    "    def arrange(self, subdim=None):\n",
    "        \"\"\"\n",
    "        Helper function to redistrubte subplots to maximize squareness or fulfill user-set dimensions\n",
    "        Input: Optional custom dimensions\n",
    "        Output: :)\n",
    "        \"\"\"\n",
    "        if not subdim == None:\n",
    "            # Throw error if there are not enough subplot spaces\n",
    "            if subdim[0] * subdim[1] < len(self.plots):\n",
    "                raise Exception('Specified plot dimensions cannot fit specidied subplot data. Try doing math')\n",
    "            self.subdim = subdim\n",
    "        else:\n",
    "            # Find the arrangement closest to square that fills all subplots\n",
    "            for i in range(int(np.sqrt(len(self.plots))), 0, -1):\n",
    "                if len(self.plots) % i == 0:\n",
    "                    self.subdim = (i, len(self.plots) // i)\n",
    "                    break\n",
    "        \n",
    "                \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Generates the figure\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        for i in range(len(self.plots)):\n",
    "            plt.subplot(self.subdim[0], self.subdim[1], i + 1)\n",
    "            self.plots[i].plot()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDriverData(weekendDF):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of driver IDs to driver stats\n",
    "    Input: Dataframe\n",
    "    Output: Dictionary of driver IDs to driver stat dataframe, \n",
    "    which is a subset of the input\n",
    "    \"\"\"\n",
    "    idList = list(set(weekendDF['Driver'].values))\n",
    "    return {ID : weekendDF.loc[weekendDF['Driver'] == ID] for ID in idList}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDataFrame = masterList.copy()\n",
    "\n",
    "# Split data into a dictionary by race\n",
    "raceNames = [race[0] for race in raceCalendar.values()]\n",
    "raceDataFrames = {race : plotDataFrame.loc[plotDataFrame[\"Weekend\"] == race] for race in raceNames}\n",
    "\n",
    "# Split dictionary of races further into to dictionaries of driver IDs to driver data\n",
    "# Get a specific stat with driverDataFrames['raceName']['driverID']['field'].values\n",
    "driverDataFrames = {name : splitDriverData(raceDataFrames[name]) for name in raceNames}\n",
    "\n",
    "# Make sorted list of driver IDs\n",
    "driverIDs = list(set([int(ID) for ID in plotDataFrame['Driver'].values]))\n",
    "driverIDs.sort()\n",
    "driverIDs = [str(ID) for ID in driverIDs]\n",
    "\n",
    "# Select races to plot\n",
    "selectedRaces = raceNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77984506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataField(field):\n",
    "    \"\"\"\n",
    "    Takes a field name and produces a dictionary of races to \n",
    "    dictionaries of drivers to numpy arrays of driver data\n",
    "    Input: field to extract\n",
    "    \"\"\"\n",
    "    dataField = {}\n",
    "    buffer = {}\n",
    "    \n",
    "    for race in selectedRaces:\n",
    "        for ID in driverIDs:\n",
    "            try:\n",
    "                arr = driverDataFrames[race][ID][field].dropna().values\n",
    "                if arr.dtype == 'timedelta64[ns]':\n",
    "                    buffer[ID] = arr.astype('float64') / 1e9\n",
    "                else:\n",
    "                    buffer[ID] = arr\n",
    "            except KeyError:\n",
    "                buffer[ID] = np.array([0])\n",
    "                \n",
    "        dataField[race] = buffer.copy()\n",
    "        buffer.clear()\n",
    "        \n",
    "    return dataField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotField(field, saveName=None):\n",
    "    \"\"\"\n",
    "    Given a field name, generates boxplots for that field for \n",
    "    each driver across each race, optionally saves an image\n",
    "    Input: field to plot, file name to save plot as\n",
    "    \"\"\"\n",
    "    # Gets nested dictionaries of field\n",
    "    dataField = getDataField(field)\n",
    "    fieldPlotData = []\n",
    "\n",
    "    # Generate list of PlotData\n",
    "    for race, driverData in dataField.items():\n",
    "        raceData = [driverField for driverField in driverData.values()]\n",
    "        title = f'{field} of drivers in {race}'\n",
    "        fieldPlotData.append(PlotData(raceData, None, 'boxplot', title, 'Drivers', field))\n",
    "        \n",
    "    # Create the figure\n",
    "    plotter = Plotter(fieldPlotData)\n",
    "    plotter.plot()\n",
    "    \n",
    "    # Save image if necessary\n",
    "    if saveName != None:\n",
    "        plt.savefig('plots/' + saveName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daec051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of field names\n",
    "timeFields = ['LapTime', 'Sector1Time', 'Sector2Time','Sector3Time']\n",
    "speedFields = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']\n",
    "environmentFields = ['AirTemp', 'Humidity', 'Pressure', 'TrackTemp', 'Windspeed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd86eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot practice data\n",
    "allFields = timeFields + speedFields + environmentFields\n",
    "for field in allFields:\n",
    "    plotField(field, field + 'training/BoxPlots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot qualifying data\n",
    "allFields = ['Qualifying' + field for field in allFields]\n",
    "for field in allFields:\n",
    "    plotField(field, field + 'qualifying/BoxPlots.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barField(field, saveName=None):\n",
    "    \"\"\"\n",
    "    Given a field name, generates boxplots for that field for \n",
    "    each driver across each race, optionally saves an image\n",
    "    Input: field to plot, file name to save plot as\n",
    "    \"\"\"\n",
    "    # Gets nested dictionaries of field\n",
    "    dataField = getDataField(field)\n",
    "    fieldPlotData = []\n",
    "\n",
    "    # Generate list of PlotData\n",
    "    for race, driverData in dataField.items():\n",
    "        raceData = [driverField[0] for driverField in driverData.values()]\n",
    "        xaxis = [[i for i in range(len(raceData))]]\n",
    "        title = f'{field} of drivers in {race}'\n",
    "        fieldPlotData.append(PlotData(xaxis, [raceData], 'bar', title, 'Drivers', field))\n",
    "        \n",
    "    # Create the figure\n",
    "    plotter = Plotter(fieldPlotData)\n",
    "    plotter.plot()\n",
    "    \n",
    "    # Save image if necessary\n",
    "    if saveName != None:\n",
    "        plt.savefig('plots/' + saveName)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f693510",
   "metadata": {},
   "outputs": [],
   "source": [
    "barField('QualifyingLapTime', 'qual')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
