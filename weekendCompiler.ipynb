{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579455ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "from fastf1 import api\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from dfply import *\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#note: you should put in a new folder to store cached data, github doesn't push empty folders so you have to do it manually\n",
    "#In the zip, FastF1Cache should have data in it so it loads faster\n",
    "fastf1.Cache.enable_cache('FastF1Cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c0de9",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QualiScrape(grandPrix, raceDate, qualiDate):\n",
    "    \"\"\"\n",
    "    Input: grandPrix, Grand Prix's Race Date, Date of Qualifying\n",
    "    Returns: data frame with each driver matched to best qualifying time, along with tire and weather conditions\n",
    "    \"\"\"\n",
    "    #Build path to scrape data from\n",
    "    Quali = fastf1.api.make_path(grandPrix, raceDate, 'Qualifying', qualiDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    QualiTiming = fastf1.api.timing_data(Quali)[0]\n",
    "    QualiTimingApp = fastf1.api.timing_app_data(Quali)\n",
    "    QualiWeatherDict = fastf1.api.weather_data(Quali)\n",
    "    QualiWeatherData = pd.DataFrame.from_dict(QualiWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    QualiTiming = QualiTiming[QualiTiming.LapTime.notnull()]\n",
    "    QualiTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    QualiTimingApp = QualiTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(QualiTimingApp)):\n",
    "        if not QualiTimingApp.loc[index,\"Compound\"]:\n",
    "            QualiTimingApp.loc[index, \"Compound\"] = QualiTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not QualiTimingApp.loc[index,\"New\"]:\n",
    "            QualiTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if QualiTimingApp.loc[index,\"Time\"] == QualiTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not QualiTimingApp.loc[index,\"LapTime\"]:\n",
    "                QualiTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                QualiTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    QualiTimingApp = QualiTimingApp[QualiTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows\n",
    "    QualiTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    QualiTiming = QualiTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "\n",
    "    QualiCompiledTiming = pd.merge(QualiTiming,QualiTimingApp, on=['Time', 'Driver'])\n",
    "\n",
    "    QualiWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    QualiCompiledTiming = pd.merge(QualiCompiledTiming, QualiWeatherData, on=['Time'])\n",
    "    QualiCompiledTiming >>= arrange(X.Driver, X.Time)\n",
    "    QualiCompiledTiming = QualiCompiledTiming.reset_index()\n",
    "    \n",
    "    #Creating List of all drivers, then finding the best lap time for them\n",
    "    driverList = QualiCompiledTiming.Driver.unique()\n",
    "    QualiTimes = {}\n",
    "    for drv in driverList:\n",
    "        QualiTimes[drv] = [drv, QualiTiming[QualiTiming.Driver == drv].LapTime.min()]\n",
    "    QualiDF = pd.DataFrame.from_dict(QualiTimes, orient = 'index', columns = ['Driver','LapTime'])\n",
    "    QualiDF = QualiDF.reset_index()\n",
    "    QualiDF >>= select(X.Driver, X.LapTime)\n",
    "    \n",
    "    #Merge weather and tire data with appropriate laptime data\n",
    "    QualiDF = pd.merge(QualiDF, QualiCompiledTiming, on = [\"LapTime\", \"Driver\"])\n",
    "    \n",
    "    #Removing variables regarded as unnecessary, adding Qualifying prefix to Data so merging stays clean\n",
    "    #Note: Qualifying and Practice are merged by driver, so QualifyingDriver is renamed back to driver\n",
    "    QualiDF >>= select(X.Driver, X.LapTime, X.Compound, X.AirTemp, X.Humidity, X.Pressure, X.Rainfall, X.TrackTemp, X.WindSpeed) \\\n",
    "            >> mutate(LapTime = X.LapTime/ timedelta(seconds = 1))\n",
    "    QualiDF = QualiDF.add_prefix('Qualifying') >> rename(Driver = X.QualifyingDriver)\n",
    "    \n",
    "    return QualiDF\n",
    "\n",
    "def practiceScrape(grandPrix, raceDate, pracNum, pracDate):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix, Grand Prix Date, Which Practice, Which Date\n",
    "    Output: Dataframe with each driver's timed lap matched with tire compound and weather conditions\n",
    "    \"\"\"\n",
    "    #Make Path to scrape practice data\n",
    "    FP = fastf1.api.make_path(grandPrix, raceDate, f'Practice {pracNum}', pracDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    FPTiming = fastf1.api.timing_data(FP)[0]\n",
    "    FPTimingApp = fastf1.api.timing_app_data(FP)\n",
    "    FPWeatherDict = fastf1.api.weather_data(FP)\n",
    "    FPWeatherData = pd.DataFrame.from_dict(FPWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    FPTiming = FPTiming[FPTiming.LapTime.notnull()]\n",
    "    FPTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    FPTimingApp = FPTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(FPTimingApp)):\n",
    "        if not FPTimingApp.loc[index,\"Compound\"]:\n",
    "            FPTimingApp.loc[index, \"Compound\"] = FPTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not FPTimingApp.loc[index,\"New\"]:\n",
    "            FPTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if FPTimingApp.loc[index,\"Time\"] == FPTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not FPTimingApp.loc[index,\"LapTime\"]:\n",
    "                FPTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                FPTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    FPTimingApp = FPTimingApp[FPTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows, and rounding FPtiming session time to nearest minute\n",
    "    FPTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    FPTiming = FPTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "    \n",
    "    #Merge Dataframes, keyed by time and driver, then dropping irrelevant columns\n",
    "    FPCompiledTiming = pd.merge(FPTiming,FPTimingApp, on=['Time', 'Driver'])\n",
    "    FPWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    FPCompiledTiming = pd.merge(FPCompiledTiming, FPWeatherData, on=['Time'])\n",
    "    FPCompiledTiming >>= arrange(X.Driver, X.Time) >> mutate(Practice = pracNum) >> drop(X.WindDirection, X.Stint, X.TotalLaps)\\\n",
    "                     >> mutate(LapTime = X.LapTime/timedelta(seconds = 1)) >> mutate(Sector1Time = X.Sector1Time/timedelta(seconds = 1)) \\\n",
    "                     >> mutate(Sector2Time = X.Sector2Time/timedelta(seconds = 1)) >> mutate(Sector3Time = X.Sector3Time/timedelta(seconds = 1))\n",
    "    FPCompiledTiming = FPCompiledTiming.reset_index()\n",
    "    return FPCompiledTiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekendCompiler(grandPrix, raceDate, sprint = False):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix with appropriate race date, and whether grand prix was a sprint weekend\n",
    "    Output: Dataframe with all timed practice laps, merged with info about best qualifying lap\n",
    "    \"\"\"\n",
    "    #Creating relevant dates for session calling purposes\n",
    "    raceDate = datetime.strptime(raceDate, \"%Y-%m-%d\")\n",
    "    friDate = str(raceDate - timedelta(days = 2))[:10]\n",
    "    satDate = str(raceDate - timedelta(days = 1))[:10]\n",
    "    raceDate = str(raceDate)[:10]\n",
    "    \n",
    "    #Collecting Practice Data\n",
    "    #Note: On sprint weekends only one practice is done before qualifying, so FP1 is the only relevant data\n",
    "    FP1CompiledTiming = practiceScrape(grandPrix, raceDate, 1, friDate)\n",
    "    if not sprint:\n",
    "        FP2CompiledTiming = practiceScrape(grandPrix, raceDate, 2, friDate)\n",
    "        FP3CompiledTiming = practiceScrape(grandPrix, raceDate, 3, satDate)\n",
    "    \n",
    "    #Collecting Quali Data:\n",
    "    #Note: On sprint weekends, qualifying is done on Friday, as opposed to usual Saturdays\n",
    "    if not sprint:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, satDate)\n",
    "    else:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, friDate)\n",
    "    \n",
    "    #Concatenating Practice Dataframes\n",
    "    if not sprint:\n",
    "        frames = [FP1CompiledTiming, FP2CompiledTiming, FP3CompiledTiming]\n",
    "        practiceCompiled = pd.concat(frames)\n",
    "    else:\n",
    "        practiceCompiled = FP1CompiledTiming\n",
    "    \n",
    "    #Merging Quali Data with compiled Practice data, such that each laptime has the appropriate quali data\n",
    "    practiceCompiled = practiceCompiled.reset_index()\n",
    "    practiceCompiled >>= drop(X.level_0)\n",
    "    practiceCompiled = pd.merge(practiceCompiled, QualiDF, on=['Driver']) >> mutate(Weekend = grandPrix)\n",
    "    \n",
    "    return practiceCompiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34b074",
   "metadata": {},
   "source": [
    "Main Code for constructing data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "raceCalendar = {1:[\"Bahrain Grand Prix\", \"2022-03-20\"], 2:[\"Saudi Arabian Grand Prix\", \"2022-03-27\"], 3:[\"Australian Grand Prix\", \"2022-04-10\"],\\\n",
    "                4:[\"Emilia Romagna Grand Prix\", \"2022-04-24\"], 5:[\"Miami Grand Prix\", \"2022-05-08\"], 6:[\"Spanish Grand Prix\", \"2022-05-22\"], \\\n",
    "                7:[\"Monaco Grand Prix\", \"2022-05-29\"], 8:[\"Azerbaijan Grand Prix\", \"2022-06-12\"], 9:[\"Canadian Grand Prix\",  \"2022-06-19\"], \\\n",
    "                10:[\"British Grand Prix\", \"2022-07-03\"], 11:[\"Austrian Grand Prix\", \"2022-07-10\"], 12:[\"French Grand Prix\", \"2022-07-24\"], \\\n",
    "                13:[\"Hungarian Grand Prix\", \"2022-07-31\"], 14:[\"Belgian Grand Prix\", \"2022-08-28\"], 15:[\"Dutch Grand Prix\", \"2022-09-04\"], \\\n",
    "                16:[\"Italian Grand Prix\", \"2022-09-11\"], 17:[\"Singapore Grand Prix\", \"2022-10-02\"], 18:[\"Japanese Grand Prix\", \"2022-10-09\"], \\\n",
    "                19:[\"United States Grand Prix\", \"2022-10-23\"], 20:[\"Mexico City Grand Prix\", \"2022-10-30\"], 21:[\"SÃ£o Paulo Grand Prix\", \"2022-11-13\"]}\n",
    "dfList = []\n",
    "for key in raceCalendar:\n",
    "    \n",
    "    print(f\"Collecting Data for {raceCalendar[key][0]}\")\n",
    "    #Imola, Austria, and Brazil are sprint weekends, so they have a special case where sprint = true\n",
    "    if key not in [4,11,21]:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1]))\n",
    "    else:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1], sprint = True))\n",
    "\n",
    "masterList = pd.concat(dfList)\n",
    "masterList >>= drop(X.Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c937f",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb0571",
   "metadata": {},
   "source": [
    "Clean Data For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63740b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn_copy = masterList.copy()\n",
    "nn_copy = pd.get_dummies(nn_copy, columns = ['Weekend', 'Driver', 'Compound', 'QualifyingCompound'])\n",
    "\n",
    "#Adjust Boolean Data\n",
    "nn_copy['New'] = pd.to_numeric(nn_copy['New'], errors = 'coerce')\n",
    "\n",
    "nn_copy = nn_copy.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training variables(X) and prediction variables (Y)\n",
    "x = nn_copy >> drop(X.QualifyingLapTime)\n",
    "             \n",
    "y = nn_copy['QualifyingLapTime']\n",
    "\n",
    "#Normalize Training Data\n",
    "\n",
    "x_scaler = preprocessing.StandardScaler().fit(x)\n",
    "x_norm = x_scaler.transform(x)\n",
    "\n",
    "#Split Data Into Test and Training Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size = 0.2)\n",
    "\n",
    "x_train = np.asarray(x_train).astype(float)\n",
    "y_train = np.asarray(y_train).astype(float)\n",
    "x_test = np.asarray(x_test).astype(float)\n",
    "y_test = np.asarray(y_test).astype(float)\n",
    "\n",
    "#Split Test Data Into Validation and Test Sets\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c1bca",
   "metadata": {},
   "source": [
    "\"Standard\" Neural Network Following General Guidelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3188566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One layer is generally enough for simple problems like this (As opposed to something like digit recognition)\n",
    "#Hidden layer size is generally between the input size (number of x variables) and the output size (number of y variables)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "\n",
    "#Adam is the recommended default optimizer\n",
    "#Mean squared error loss function as seen in class\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "#Start Batch Size Small (Powers of 2 for efficient GPU usage)\n",
    "#Epochs rule of thumb is to start with triple the amount of columns\n",
    "#Verbose determines how it prints the training progress\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)\n",
    "\n",
    "#Evaluate Model \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd464f54",
   "metadata": {},
   "source": [
    "What If We Adjust The Network Architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f33a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e230a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is best\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate best model with test data\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7496c07",
   "metadata": {},
   "source": [
    "What if we change the activation function? (They are all worse than relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'linear'),\n",
    "    tf.keras.layers.Dense(16, activation = 'linear'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99571dcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(16, activation = 'sigmoid'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(16, activation = 'tanh'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7dcbf",
   "metadata": {},
   "source": [
    "What if we adjust batch size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74418d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 64, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d007e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is superior\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 8, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is even more superior (Not going smaller because run time is very long)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 4, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52939d",
   "metadata": {},
   "source": [
    "What if we add more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Much better but takes a long time to train and could cause overfitting (Best model with all the data)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                   tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1000, verbose=2)\n",
    "model.evaluate(x_validation, y_validation, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce465a13",
   "metadata": {},
   "source": [
    "Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837079e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39386aa7",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genModel(layers, batchSize=32, epochs=100, verbose=2):\n",
    "    \"\"\"\n",
    "    Automated neural network generator\n",
    "    Input: list of layers, batch size, number of epochs, verbosity\n",
    "    Output: it do the thing\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential(layers)\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                   tf.keras.metrics.MeanAbsoluteError()])\n",
    "    model.fit(x_train, y_train, batchSize, epochs, verbose)\n",
    "    model.evaluate(x_validation, y_validation, verbose)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "class PlotData():\n",
    "    \"\"\"\n",
    "    A module for storing plot data in a more portable, reusable form\n",
    "    Stores x and y data and metadata (title, labels, etc.)\n",
    "    Parameters: 2 arrays of x and y datasets (to allow multiple relations per plot), \n",
    "                plot type as a string, title, axis labels, and grid boolean\n",
    "    \"\"\"\n",
    "    def __init__(self, xdata, ydata, ptype='plot', title='Figure', xlabel='x axis', ylabel='y axis', grid=True):\n",
    "        \n",
    "        # Only check these if we have ydata\n",
    "        if ydata != None:\n",
    "            # Throws error if xdata and ydata length do not match\n",
    "            if not len(xdata) == len(ydata):\n",
    "                raise Exception('Data mismatch error: must have the same number of corresponding x and y fields')\n",
    "            \n",
    "            # Throws error if x and y subdata length do not match\n",
    "                for i in range(len(xdata)):\n",
    "                    if not len(xdata[i]) == len(ydata[i]):\n",
    "                        raise Exception(f'Data mismatch error: {i}th set of x and y data have unequal length')\n",
    "        \n",
    "            \n",
    "        self.xdata = xdata\n",
    "        self.ydata = ydata\n",
    "        self.ptype = ptype\n",
    "        self.title = title\n",
    "        self.xlabel = xlabel\n",
    "        self.ylabel = ylabel\n",
    "        self.grid = grid\n",
    "        \n",
    "        \n",
    "    def plot(self, style='b--'):\n",
    "        \"\"\"\n",
    "        May not be showing it but it do be plotting the data\n",
    "        Input: Optional style\n",
    "        \"\"\"\n",
    "        # Plots based on type\n",
    "        if self.ptype == 'plot':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.plot(self.xdata[i], self.ydata[i], style)\n",
    "                \n",
    "        elif self.ptype == 'scatter':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.scatter(self.xdata[i], self.ydata[i], style)\n",
    "            \n",
    "        elif self.ptype == 'bar':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.bar(self.xdata[i], self.ydata[i])\n",
    "                \n",
    "        elif self.ptype == 'boxplot':\n",
    "            plt.boxplot(self.xdata)\n",
    "            \n",
    "        else:\n",
    "            print(f'{self.title} has invalid plot type')\n",
    "                \n",
    "        plt.title(self.title)\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if self.grid:\n",
    "            plt.grid()\n",
    "            \n",
    "            \n",
    "    def addData(x, y):\n",
    "        \"\"\"\n",
    "        Clears the figure and adds a new x-y relation to the data\n",
    "        Input: x and y data to add\n",
    "        Output: It adds the data wow\n",
    "        \"\"\"\n",
    "        plt.clf()\n",
    "        self.xdata.append(x)\n",
    "        self.ydata.append(y)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35125264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter():\n",
    "    \"\"\"\n",
    "    Takes plot data and auto-generates subplots\n",
    "    Parameters: a list of PlotData objects\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, plots, subdim=None):\n",
    "        self.plots = plots\n",
    "        self.subdim = subdim\n",
    "        self.arrange(self.subdim)\n",
    "        self.figsize = (5*self.subdim[1], 5*self.subdim[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def addPlot(plot, loc=-1, subdim=None):\n",
    "        \"\"\"\n",
    "        Adds a new subplot and rearranges the figure\n",
    "        Input: the PlotData to add, the location to display it, and a custom aspect ratio if desired\n",
    "        Output:  It does it\n",
    "        \"\"\"\n",
    "        self.plots.insert(loc, plot)\n",
    "        if not subdim == None:\n",
    "            self.subdim = subdim\n",
    "        else:\n",
    "            arrange()\n",
    "    \n",
    "    \n",
    "    def addData(x, y, loc):\n",
    "        \"\"\"\n",
    "        Adds data to a subplot\n",
    "        Input: x and y data to add, and which subplot to add to\n",
    "        Output: It also does it\n",
    "        \"\"\"\n",
    "        self.plots[loc].addData(x, y)\n",
    "    \n",
    "    \n",
    "    def arrange(self, subdim=None):\n",
    "        \"\"\"\n",
    "        Helper function to redistrubte subplots to maximize squareness or fulfill user-set dimensions\n",
    "        Input: Optional custom dimensions\n",
    "        Output: :)\n",
    "        \"\"\"\n",
    "        if not subdim == None:\n",
    "            # Throw error if there are not enough subplot spaces\n",
    "            if subdim[0] * subdim[1] < len(self.plots):\n",
    "                raise Exception('Specified plot dimensions cannot fit specidied subplot data. Try doing math')\n",
    "            self.subdim = subdim\n",
    "        else:\n",
    "            # Find the arrangement closest to square that fills all subplots\n",
    "            for i in range(int(np.sqrt(len(self.plots))), 0, -1):\n",
    "                if len(self.plots) % i == 0:\n",
    "                    self.subdim = (i, len(self.plots) // i)\n",
    "                    break\n",
    "        \n",
    "                \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Generates the figure\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        for i in range(len(self.plots)):\n",
    "            plt.subplot(self.subdim[0], self.subdim[1], i + 1)\n",
    "            self.plots[i].plot()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDrivers(weekendDF):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of driver IDs to driver stats\n",
    "    Input: Dataframe\n",
    "    Output: Dictionary of driver IDs to driver stat dataframe (which is a subset of the input)\n",
    "    \"\"\"\n",
    "    idList = list(set(weekendDF['Driver'].values))\n",
    "    return {ID : weekendDF.loc[weekendDF['Driver'] == ID] for ID in idList}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539912fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverages(dataframe, race, driverIDs, category):\n",
    "    \"\"\"\n",
    "    Generates a list of average values for a driver stat in a given race\n",
    "    Input: Dictionary of races to dictionaries of IDs to driver dataframes, the race to \n",
    "           select, the driver to select, and the driver stat to average\n",
    "    Output: The mean value\n",
    "    \"\"\"\n",
    "    \n",
    "    averages = []\n",
    "    for ID in driverIDs:\n",
    "        # If the driver is not present they get 0\n",
    "        try:\n",
    "            averages.append(dataframe[race][ID][category].values.mean().item() / 1.0e9)\n",
    "        except:\n",
    "            averages.append(0)\n",
    "    return averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim down to just the data we're interested in\n",
    "categories = ['Driver', 'LapTime', 'Sector1Time', 'Sector2Time','Sector3Time', 'Compound', 'QualifyingLapTime', \n",
    "          'QualifyingSector1Time', 'QualifyingSector2Time','QualifyingSector3Time', 'QualifyingCompound', 'Weekend']\n",
    "plotDataFrame = masterList.copy()\n",
    "\n",
    "# Split data into a dictionary by race\n",
    "raceNames = [race[0] for race in raceCalendar.values()]\n",
    "raceDataFrames = {race : plotDataFrame.loc[plotDataFrame[\"Weekend\"] == race] for race in raceNames}\n",
    "\n",
    "# Split dictionary of races further into to dictionaries of driver IDs to driver data\n",
    "# Get a specific stat with driverDataFrames['raceName']['driverID']['field'].values\n",
    "driverDataFrames = {name : genDrivers(raceDataFrames[name]) for name in raceNames}\n",
    "\n",
    "# Make sorted list of driver IDs\n",
    "driverIDs = list(set([int(ID) for ID in plotDF['Driver'].values]))\n",
    "driverIDs.sort()\n",
    "driverIDs = [str(ID) for ID in driverIDs]\n",
    "\n",
    "# Select races to plot\n",
    "selectedRaces = raceNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77984506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataField(field):\n",
    "    \"\"\"\n",
    "    Takes a field name and produces a dictionary of races to dictionaries of drivers to numpy arrays of driver data\n",
    "    \"\"\"\n",
    "    dataField = {}\n",
    "    buffer = {}\n",
    "    \n",
    "    for race in selectedRaces:\n",
    "        for ID in driverIDs:\n",
    "            try:\n",
    "                arr = driverDataFrames[race][ID][field].values\n",
    "                if arr.dtype == 'timedelta64[ns]':\n",
    "                    buffer[ID] = arr.astype('float64') / 1e9\n",
    "                else:\n",
    "                    buffer[ID] = arr\n",
    "            except:\n",
    "                buffer[ID] = np.array([0])\n",
    "                \n",
    "        dataField[race] = buffer.copy()\n",
    "        buffer.clear()\n",
    "        \n",
    "    return dataField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotField(field, saveName=None):\n",
    "    \"\"\"\n",
    "    Given a field name, generates boxplots for that field for each driver across each race, optionally saves an image\n",
    "    \"\"\"\n",
    "    lapTimes = getDataField(field)\n",
    "    fieldPlotData = []\n",
    "\n",
    "    for race, driverData in lapTimes.items():\n",
    "        raceData = [driverField for driverField in driverData.values()]\n",
    "        title = f'{field} of drivers in {race}'\n",
    "        fieldPlotData.append(PlotData(raceData, None, 'boxplot', title, 'Drivers', field))\n",
    "        \n",
    "    plotter = Plotter(fieldPlotData)\n",
    "    plotter.plot()\n",
    "    \n",
    "    if saveName != None:\n",
    "        plt.savefig(saveName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daec051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots and saves each dataframe field\n",
    "# FIXME: plots for some races are mostly blank or show negative values?\n",
    "timeFields = ['LapTime', 'Sector1Time', 'Sector2Time','Sector3Time']\n",
    "speedFields = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']\n",
    "environmentFields = ['AirTemp', 'Humidity', 'Pressure', 'TrackTemp', 'Windspeed']\n",
    "\n",
    "qtimeFields = ['QualifyingLapTime', 'QualifyingSector1Time', 'QualifyingSector2Time','QualifyingSector3Time']\n",
    "qspeedFields = ['QualifyingSpeedI1', 'QualifyingSpeedI2', 'QualifyingSpeedFL', 'QualifyingSpeedST']\n",
    "\n",
    "for field in speedFields:\n",
    "    plotField(field, field + 'BoxPlots.png')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
