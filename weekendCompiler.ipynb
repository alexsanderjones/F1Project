{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "import fastf1.api\n",
    "import pandas as pd\n",
    "from dfply import *\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#note: you should put in a new folder to store cached data, github doesn't push empty folders so you have to do it manually\n",
    "fastf1.Cache.enable_cache('FastF1Cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QualiScrape(grandPrix, raceDate, qualiDate):\n",
    "    \"\"\"\n",
    "    Input: grandPrix, Grand Prix's Race Date, Date of Qualifying\n",
    "    Returns: data frame with each driver matched to best qualifying time, along with tire and weather conditions\n",
    "    \"\"\"\n",
    "    #Build path to scrape data from\n",
    "    Quali = fastf1.api.make_path(grandPrix, raceDate, 'Qualifying', qualiDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    QualiTiming = fastf1.api.timing_data(Quali)[0]\n",
    "    QualiTimingApp = fastf1.api.timing_app_data(Quali)\n",
    "    QualiWeatherDict = fastf1.api.weather_data(Quali)\n",
    "    QualiWeatherData = pd.DataFrame.from_dict(QualiWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    QualiTiming = QualiTiming[QualiTiming.LapTime.notnull()]\n",
    "    QualiTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    QualiTimingApp = QualiTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(QualiTimingApp)):\n",
    "        if not QualiTimingApp.loc[index,\"Compound\"]:\n",
    "            QualiTimingApp.loc[index, \"Compound\"] = QualiTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not QualiTimingApp.loc[index,\"New\"]:\n",
    "            QualiTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if QualiTimingApp.loc[index,\"Time\"] == QualiTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not QualiTimingApp.loc[index,\"LapTime\"]:\n",
    "                QualiTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                QualiTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    QualiTimingApp = QualiTimingApp[QualiTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows\n",
    "    QualiTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    QualiTiming = QualiTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "\n",
    "    QualiCompiledTiming = pd.merge(QualiTiming,QualiTimingApp, on=['Time', 'Driver'])\n",
    "\n",
    "    QualiWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    QualiCompiledTiming = pd.merge(QualiCompiledTiming, QualiWeatherData, on=['Time'])\n",
    "    QualiCompiledTiming >>= arrange(X.Driver, X.Time)\n",
    "    QualiCompiledTiming = QualiCompiledTiming.reset_index()\n",
    "    \n",
    "    #Creating List of all drivers, then finding the best lap time for them\n",
    "    driverList = QualiCompiledTiming.Driver.unique()\n",
    "    QualiTimes = {}\n",
    "    for drv in driverList:\n",
    "        QualiTimes[drv] = [drv, QualiTiming[QualiTiming.Driver == drv].LapTime.min()]\n",
    "    QualiDF = pd.DataFrame.from_dict(QualiTimes, orient = 'index', columns = ['Driver','LapTime'])\n",
    "    QualiDF = QualiDF.reset_index()\n",
    "    QualiDF >>= select(X.Driver, X.LapTime)\n",
    "    \n",
    "    #Merge weather and tire data with appropriate laptime data\n",
    "    QualiDF = pd.merge(QualiDF, QualiCompiledTiming, on = [\"LapTime\", \"Driver\"])\n",
    "    \n",
    "    #Removing variables regarded as unnecessary, adding Qualifying prefix to Data so merging stays clean\n",
    "    #Note: Qualifying and Practice are merged by driver, so QualifyingDriver is renamed back to driver\n",
    "    QualiDF >>= drop(X.Time, X.NumberOfLaps, X.NumberOfPitStops, X.IsPersonalBest, X.Stint, X.TotalLaps, X.New, X.WindDirection)\n",
    "    QualiDF = QualiDF.add_prefix('Qualifying') >> rename(Driver = X.QualifyingDriver)\n",
    "    QualiDF >>= drop(X.Qualifyingindex)\n",
    "    \n",
    "    return QualiDF\n",
    "\n",
    "def practiceScrape(grandPrix, raceDate, pracNum, pracDate):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix, Grand Prix Date, Which Practice, Which Date\n",
    "    Output: Dataframe with each driver's timed lap matched with tire compound and weather conditions\n",
    "    \"\"\"\n",
    "    #Make Path to scrape practice data\n",
    "    FP = fastf1.api.make_path(grandPrix, raceDate, f'Practice {pracNum}', pracDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    FPTiming = fastf1.api.timing_data(FP)[0]\n",
    "    FPTimingApp = fastf1.api.timing_app_data(FP)\n",
    "    FPWeatherDict = fastf1.api.weather_data(FP)\n",
    "    FPWeatherData = pd.DataFrame.from_dict(FPWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    FPTiming = FPTiming[FPTiming.LapTime.notnull()]\n",
    "    FPTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    FPTimingApp = FPTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(FPTimingApp)):\n",
    "        if not FPTimingApp.loc[index,\"Compound\"]:\n",
    "            FPTimingApp.loc[index, \"Compound\"] = FPTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not FPTimingApp.loc[index,\"New\"]:\n",
    "            FPTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if FPTimingApp.loc[index,\"Time\"] == FPTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not FPTimingApp.loc[index,\"LapTime\"]:\n",
    "                FPTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                FPTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    FPTimingApp = FPTimingApp[FPTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows, and rounding FPtiming session time to nearest minute\n",
    "    FPTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    FPTiming = FPTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "    \n",
    "    #Merge Dataframes, keyed by time and driver, then dropping irrelevant columns\n",
    "    FPCompiledTiming = pd.merge(FPTiming,FPTimingApp, on=['Time', 'Driver'])\n",
    "    FPWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    FPCompiledTiming = pd.merge(FPCompiledTiming, FPWeatherData, on=['Time'])\n",
    "    FPCompiledTiming >>= arrange(X.Driver, X.Time) >> mutate(Practice = pracNum) >> drop(X.WindDirection, X.Stint, X.TotalLaps)\n",
    "    FPCompiledTiming = FPCompiledTiming.reset_index()\n",
    "    return FPCompiledTiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekendCompiler(grandPrix, raceDate, sprint = False):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix with appropriate race date, and whether grand prix was a sprint weekend\n",
    "    Output: Dataframe with all timed practice laps, merged with info about best qualifying lap\n",
    "    \"\"\"\n",
    "    #Creating relevant dates for session calling purposes\n",
    "    raceDate = datetime.strptime(raceDate, \"%Y-%m-%d\")\n",
    "    friDate = str(raceDate - timedelta(days = 2))[:10]\n",
    "    satDate = str(raceDate - timedelta(days = 1))[:10]\n",
    "    raceDate = str(raceDate)[:10]\n",
    "    \n",
    "    #Collecting Practice Data\n",
    "    #Note: On sprint weekends only one practice is done before qualifying, so FP1 is the only relevant data\n",
    "    FP1CompiledTiming = practiceScrape(grandPrix, raceDate, 1, friDate)\n",
    "    if not sprint:\n",
    "        FP2CompiledTiming = practiceScrape(grandPrix, raceDate, 2, friDate)\n",
    "        FP3CompiledTiming = practiceScrape(grandPrix, raceDate, 3, satDate)\n",
    "    \n",
    "    #Collecting Quali Data:\n",
    "    #Note: On sprint weekends, qualifying is done on Friday, as opposed to usual Saturdays\n",
    "    if not sprint:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, satDate)\n",
    "    else:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, friDate)\n",
    "    \n",
    "    #Concatenating Practice Dataframes\n",
    "    if not sprint:\n",
    "        frames = [FP1CompiledTiming, FP2CompiledTiming, FP3CompiledTiming]\n",
    "        practiceCompiled = pd.concat(frames)\n",
    "    else:\n",
    "        practiceCompiled = FP1CompiledTiming\n",
    "    \n",
    "    #Merging Quali Data with compiled Practice data, such that each laptime has the appropriate quali data\n",
    "    practiceCompiled = practiceCompiled.reset_index()\n",
    "    practiceCompiled >>= drop(X.level_0)\n",
    "    practiceCompiled = pd.merge(practiceCompiled, QualiDF, on=['Driver']) >> mutate(Weekend = grandPrix)\n",
    "    \n",
    "    return practiceCompiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "raceCalendar = {1:[\"Bahrain Grand Prix\", \"2022-03-20\"], 2:[\"Saudi Arabian Grand Prix\", \"2022-03-27\"], 3:[\"Australian Grand Prix\", \"2022-04-10\"],\\\n",
    "                4:[\"Emilia Romagna Grand Prix\", \"2022-04-24\"], 5:[\"Miami Grand Prix\", \"2022-05-08\"], 6:[\"Spanish Grand Prix\", \"2022-05-22\"], \\\n",
    "                7:[\"Monaco Grand Prix\", \"2022-05-29\"], 8:[\"Azerbaijan Grand Prix\", \"2022-06-12\"], 9:[\"Canadian Grand Prix\",  \"2022-06-19\"], \\\n",
    "                10:[\"British Grand Prix\", \"2022-07-03\"], 11:[\"Austrian Grand Prix\", \"2022-07-10\"], 12:[\"French Grand Prix\", \"2022-07-24\"], \\\n",
    "                13:[\"Hungarian Grand Prix\", \"2022-07-31\"], 14:[\"Belgian Grand Prix\", \"2022-08-28\"], 15:[\"Dutch Grand Prix\", \"2022-09-04\"], \\\n",
    "                16:[\"Italian Grand Prix\", \"2022-09-11\"], 17:[\"Singapore Grand Prix\", \"2022-10-02\"], 18:[\"Japanese Grand Prix\", \"2022-10-09\"], \\\n",
    "                19:[\"United States Grand Prix\", \"2022-10-23\"], 20:[\"Mexico City Grand Prix\", \"2022-10-30\"], 21:[\"SÃ£o Paulo Grand Prix\", \"2022-11-13\"]}\n",
    "dfList = []\n",
    "for key in raceCalendar:\n",
    "    \n",
    "    print(f\"Collecting Data for {raceCalendar[key][0]}\")\n",
    "    #Imola, Austria, and Brazil are sprint weekends, so they have a special case where sprint = true\n",
    "    if key not in [4,11,21]:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1]))\n",
    "    else:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1], sprint = True))\n",
    "\n",
    "masterList = pd.concat(dfList)\n",
    "masterList >>= drop(X.Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb0571",
   "metadata": {},
   "source": [
    "Clean Data For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63740b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn_copy = masterList.copy()\n",
    "nn_copy = pd.get_dummies(nn_copy, columns = ['Weekend', 'Driver', 'Compound', 'QualifyingCompound'])\n",
    "\n",
    "#Convert to Seconds \n",
    "nn_copy['LapTime'] = (pd.to_numeric(nn_copy['LapTime'], errors = 'coerce')) / 1000000000\n",
    "nn_copy['Sector1Time'] = (pd.to_numeric(nn_copy['Sector1Time'], errors = 'coerce')) / 1000000000\n",
    "nn_copy['Sector2Time'] = (pd.to_numeric(nn_copy['Sector2Time'], errors = 'coerce')) / 1000000000\n",
    "nn_copy['Sector3Time'] = (pd.to_numeric(nn_copy['Sector3Time'], errors = 'coerce')) / 1000000000\n",
    "nn_copy['QualifyingLapTime'] = (pd.to_numeric(nn_copy['QualifyingLapTime'], errors = 'coerce')) / 1000000000\n",
    "\n",
    "#Adjust Boolean Data\n",
    "nn_copy['New'] = pd.to_numeric(nn_copy['New'], errors = 'coerce')\n",
    "\n",
    "#Remove Bad Data\n",
    "del nn_copy['index']\n",
    "del nn_copy['QualifyingSector1Time']\n",
    "del nn_copy['QualifyingSector2Time']\n",
    "del nn_copy['QualifyingSector3Time']\n",
    "del nn_copy['QualifyingSpeedI1']\n",
    "del nn_copy['QualifyingSpeedI2']\n",
    "del nn_copy['QualifyingSpeedFL']\n",
    "del nn_copy['QualifyingSpeedST']\n",
    "nn_copy = nn_copy.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training variables(X) and prediction variables (Y)\n",
    "x = nn_copy >> drop(X.QualifyingLapTime)\n",
    "             \n",
    "y = nn_copy['QualifyingLapTime']\n",
    "\n",
    "#Normalize Training Data\n",
    "\n",
    "x_scaler = preprocessing.StandardScaler().fit(x)\n",
    "x_norm = x_scaler.transform(x)\n",
    "\n",
    "#Split Data Into Test and Training Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size = 0.2)\n",
    "\n",
    "x_train = np.asarray(x_train).astype(float)\n",
    "y_train = np.asarray(y_train).astype(float)\n",
    "x_test = np.asarray(x_test).astype(float)\n",
    "y_test = np.asarray(y_test).astype(float)\n",
    "\n",
    "#Split Test Data Into Validation and Test Sets\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c1bca",
   "metadata": {},
   "source": [
    "\"Standard\" Neural Network Following General Guidelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3188566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One layer is generally enough for simple problems like this (As opposed to something like digit recognition)\n",
    "#Hidden layer size is generally between the input size (number of x variables) and the output size (number of y variables)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "\n",
    "#Adam is the recommended default optimizer\n",
    "#Mean squared error loss function as seen in class\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "#Start Batch Size Small (Powers of 2 for efficient GPU usage)\n",
    "#Epochs rule of thumb is to start with triple the amount of columns\n",
    "#Verbose determines how it prints the training progress\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)\n",
    "\n",
    "#Evaluate Model \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd464f54",
   "metadata": {},
   "source": [
    "What If We Adjust The Network Architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f33a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e230a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is best\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate best model with test data\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7496c07",
   "metadata": {},
   "source": [
    "What if we change the activation function? (They are all worse than relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'linear'),\n",
    "    tf.keras.layers.Dense(16, activation = 'linear'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99571dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(16, activation = 'sigmoid'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(16, activation = 'tanh'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7dcbf",
   "metadata": {},
   "source": [
    "What if we adjust batch size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74418d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 64, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d007e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is superior\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 8, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is even more superior (Not going smaller because run time is very long)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 4, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52939d",
   "metadata": {},
   "source": [
    "What if we add more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Much better but takes a long time to train and could cause overfitting (Best model with all the data)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                   tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size=4, epochs=1000, verbose=2)\n",
    "model.evaluate(x_validation, y_validation, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
