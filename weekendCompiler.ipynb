{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579455ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "from fastf1 import api\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from dfply import *\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#note: you should put in a new folder to store cached data, github doesn't push empty folders so you have to do it manually\n",
    "#In the zip, FastF1Cache should have data in it so it loads faster\n",
    "fastf1.Cache.enable_cache('FastF1Cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c0de9",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e5c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QualiScrape(grandPrix, raceDate, qualiDate):\n",
    "    \"\"\"\n",
    "    Input: grandPrix, Grand Prix's Race Date, Date of Qualifying\n",
    "    Returns: data frame with each driver matched to best qualifying time, along with tire and weather conditions\n",
    "    \"\"\"\n",
    "    #Build path to scrape data from\n",
    "    Quali = fastf1.api.make_path(grandPrix, raceDate, 'Qualifying', qualiDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    QualiTiming = fastf1.api.timing_data(Quali)[0]\n",
    "    QualiTimingApp = fastf1.api.timing_app_data(Quali)\n",
    "    QualiWeatherDict = fastf1.api.weather_data(Quali)\n",
    "    QualiWeatherData = pd.DataFrame.from_dict(QualiWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    QualiTiming = QualiTiming[QualiTiming.LapTime.notnull()]\n",
    "    QualiTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    QualiTimingApp = QualiTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(QualiTimingApp)):\n",
    "        if not QualiTimingApp.loc[index,\"Compound\"]:\n",
    "            QualiTimingApp.loc[index, \"Compound\"] = QualiTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not QualiTimingApp.loc[index,\"New\"]:\n",
    "            QualiTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if QualiTimingApp.loc[index,\"Time\"] == QualiTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not QualiTimingApp.loc[index,\"LapTime\"]:\n",
    "                QualiTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                QualiTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    QualiTimingApp = QualiTimingApp[QualiTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows\n",
    "    QualiTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    QualiTiming = QualiTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "\n",
    "    QualiCompiledTiming = pd.merge(QualiTiming,QualiTimingApp, on=['Time', 'Driver'])\n",
    "\n",
    "    QualiWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    QualiCompiledTiming = pd.merge(QualiCompiledTiming, QualiWeatherData, on=['Time'])\n",
    "    QualiCompiledTiming >>= arrange(X.Driver, X.Time)\n",
    "    QualiCompiledTiming = QualiCompiledTiming.reset_index()\n",
    "    \n",
    "    #Creating List of all drivers, then finding the best lap time for them\n",
    "    driverList = QualiCompiledTiming.Driver.unique()\n",
    "    QualiTimes = {}\n",
    "    for drv in driverList:\n",
    "        QualiTimes[drv] = [drv, QualiTiming[QualiTiming.Driver == drv].LapTime.min()]\n",
    "    QualiDF = pd.DataFrame.from_dict(QualiTimes, orient = 'index', columns = ['Driver','LapTime'])\n",
    "    QualiDF = QualiDF.reset_index()\n",
    "    QualiDF >>= select(X.Driver, X.LapTime)\n",
    "    \n",
    "    #Merge weather and tire data with appropriate laptime data\n",
    "    QualiDF = pd.merge(QualiDF, QualiCompiledTiming, on = [\"LapTime\", \"Driver\"])\n",
    "    \n",
    "    #Removing variables regarded as unnecessary, adding Qualifying prefix to Data so merging stays clean\n",
    "    #Note: Qualifying and Practice are merged by driver, so QualifyingDriver is renamed back to driver\n",
    "    QualiDF >>= select(X.Driver, X.LapTime, X.Compound, X.AirTemp, X.Humidity, X.Pressure, X.Rainfall, X.TrackTemp, X.WindSpeed) \\\n",
    "            >> mutate(LapTime = X.LapTime/ timedelta(seconds = 1))\n",
    "    QualiDF = QualiDF.add_prefix('Qualifying') >> rename(Driver = X.QualifyingDriver)\n",
    "    \n",
    "    return QualiDF\n",
    "\n",
    "def practiceScrape(grandPrix, raceDate, pracNum, pracDate):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix, Grand Prix Date, Which Practice, Which Date\n",
    "    Output: Dataframe with each driver's timed lap matched with tire compound and weather conditions\n",
    "    \"\"\"\n",
    "    #Make Path to scrape practice data\n",
    "    FP = fastf1.api.make_path(grandPrix, raceDate, f'Practice {pracNum}', pracDate)\n",
    "    \n",
    "    #Scraping timing data, timing app data (for info on tire compund), and weather data\n",
    "    FPTiming = fastf1.api.timing_data(FP)[0]\n",
    "    FPTimingApp = fastf1.api.timing_app_data(FP)\n",
    "    FPWeatherDict = fastf1.api.weather_data(FP)\n",
    "    FPWeatherData = pd.DataFrame.from_dict(FPWeatherDict)\n",
    "    \n",
    "    #Remove entries with no laptimes, and round session time to nearest minute (for merging purposes)\n",
    "    FPTiming = FPTiming[FPTiming.LapTime.notnull()]\n",
    "    FPTimingApp >>= mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time)\n",
    "    FPTimingApp = FPTimingApp.reset_index()\n",
    "    \n",
    "    #In timing app dataframe, tire compound is only noted when new set is put on\n",
    "    #Since dataframe is organized by driver and then time, we can replace empty entries with the previous tire compound\n",
    "    #Also, dataframe sometime doubles on driver-time combo, so removing the unnecessary ones\n",
    "    for index in range(len(FPTimingApp)):\n",
    "        if not FPTimingApp.loc[index,\"Compound\"]:\n",
    "            FPTimingApp.loc[index, \"Compound\"] = FPTimingApp.loc[index-1, \"Compound\"]\n",
    "        if not FPTimingApp.loc[index,\"New\"]:\n",
    "            FPTimingApp.loc[index,\"New\"] = False\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if FPTimingApp.loc[index,\"Time\"] == FPTimingApp.loc[index-1,\"Time\"]:\n",
    "            if not FPTimingApp.loc[index,\"LapTime\"]:\n",
    "                FPTimingApp.loc[index,\"Driver\"] = np.nan\n",
    "            else: \n",
    "                FPTimingApp.loc[index-1,\"Driver\"] = np.nan                \n",
    "    FPTimingApp = FPTimingApp[FPTimingApp.Driver.notnull()]\n",
    "    \n",
    "    #Selecting relevant rows, and rounding FPtiming session time to nearest minute\n",
    "    FPTimingApp >>= select(X.Stint, X.Driver, X.TotalLaps, X.Compound, X.New, X.Time)\n",
    "    FPTiming = FPTiming >> mutate(Time = X.Time.round('60s')) >> arrange(X.Driver, X.Time) >> \\\n",
    "              drop(contains(\"Session\"), X.PitOutTime, X.PitInTime)\n",
    "    \n",
    "    #Merge Dataframes, keyed by time and driver, then dropping irrelevant columns\n",
    "    FPCompiledTiming = pd.merge(FPTiming,FPTimingApp, on=['Time', 'Driver'])\n",
    "    FPWeatherData >>= mutate(Time = X.Time.round('60s'))\n",
    "    FPCompiledTiming = pd.merge(FPCompiledTiming, FPWeatherData, on=['Time'])\n",
    "    FPCompiledTiming >>= arrange(X.Driver, X.Time) >> mutate(Practice = pracNum) >> drop(X.WindDirection, X.Stint, X.TotalLaps)\\\n",
    "                     >> mutate(LapTime = X.LapTime/timedelta(seconds = 1)) >> mutate(Sector1Time = X.Sector1Time/timedelta(seconds = 1)) \\\n",
    "                     >> mutate(Sector2Time = X.Sector2Time/timedelta(seconds = 1)) >> mutate(Sector3Time = X.Sector3Time/timedelta(seconds = 1))\n",
    "    FPCompiledTiming = FPCompiledTiming.reset_index()\n",
    "    return FPCompiledTiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5b9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekendCompiler(grandPrix, raceDate, sprint = False):\n",
    "    \"\"\"\n",
    "    Input: Grand Prix with appropriate race date, and whether grand prix was a sprint weekend\n",
    "    Output: Dataframe with all timed practice laps, merged with info about best qualifying lap\n",
    "    \"\"\"\n",
    "    #Creating relevant dates for session calling purposes\n",
    "    raceDate = datetime.strptime(raceDate, \"%Y-%m-%d\")\n",
    "    friDate = str(raceDate - timedelta(days = 2))[:10]\n",
    "    satDate = str(raceDate - timedelta(days = 1))[:10]\n",
    "    raceDate = str(raceDate)[:10]\n",
    "    \n",
    "    #Collecting Practice Data\n",
    "    #Note: On sprint weekends only one practice is done before qualifying, so FP1 is the only relevant data\n",
    "    FP1CompiledTiming = practiceScrape(grandPrix, raceDate, 1, friDate)\n",
    "    if not sprint:\n",
    "        FP2CompiledTiming = practiceScrape(grandPrix, raceDate, 2, friDate)\n",
    "        FP3CompiledTiming = practiceScrape(grandPrix, raceDate, 3, satDate)\n",
    "    \n",
    "    #Collecting Quali Data:\n",
    "    #Note: On sprint weekends, qualifying is done on Friday, as opposed to usual Saturdays\n",
    "    if not sprint:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, satDate)\n",
    "    else:\n",
    "        QualiDF = QualiScrape(grandPrix, raceDate, friDate)\n",
    "    \n",
    "    #Concatenating Practice Dataframes\n",
    "    if not sprint:\n",
    "        frames = [FP1CompiledTiming, FP2CompiledTiming, FP3CompiledTiming]\n",
    "        practiceCompiled = pd.concat(frames)\n",
    "    else:\n",
    "        practiceCompiled = FP1CompiledTiming\n",
    "    \n",
    "    #Merging Quali Data with compiled Practice data, such that each laptime has the appropriate quali data\n",
    "    practiceCompiled = practiceCompiled.reset_index()\n",
    "    practiceCompiled >>= drop(X.level_0)\n",
    "    practiceCompiled = pd.merge(practiceCompiled, QualiDF, on=['Driver']) >> mutate(Weekend = grandPrix)\n",
    "    \n",
    "    return practiceCompiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34b074",
   "metadata": {},
   "source": [
    "Main Code for constructing data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73de0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Bahrain Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Saudi Arabian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Australian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Emilia Romagna Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Miami Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Spanish Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Monaco Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Azerbaijan Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Canadian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for British Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Austrian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for French Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Hungarian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Belgian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Dutch Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Italian Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Singapore Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Japanese Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for United States Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for Mexico City Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n",
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Data for São Paulo Grand Prix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "api            INFO \tUsing cached data for timing_data\n",
      "api            INFO \tUsing cached data for timing_app_data\n",
      "api            INFO \tUsing cached data for weather_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Driver', 'LapTime', 'NumberOfLaps', 'NumberOfPitStops',\n",
       "       'Sector1Time', 'Sector2Time', 'Sector3Time', 'SpeedI1', 'SpeedI2',\n",
       "       'SpeedFL', 'SpeedST', 'IsPersonalBest', 'Compound', 'New', 'AirTemp',\n",
       "       'Humidity', 'Pressure', 'Rainfall', 'TrackTemp', 'WindSpeed',\n",
       "       'Practice', 'QualifyingLapTime', 'QualifyingCompound',\n",
       "       'QualifyingAirTemp', 'QualifyingHumidity', 'QualifyingPressure',\n",
       "       'QualifyingRainfall', 'QualifyingTrackTemp', 'QualifyingWindSpeed',\n",
       "       'Weekend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raceCalendar = {1:[\"Bahrain Grand Prix\", \"2022-03-20\"], 2:[\"Saudi Arabian Grand Prix\", \"2022-03-27\"], 3:[\"Australian Grand Prix\", \"2022-04-10\"],\\\n",
    "                4:[\"Emilia Romagna Grand Prix\", \"2022-04-24\"], 5:[\"Miami Grand Prix\", \"2022-05-08\"], 6:[\"Spanish Grand Prix\", \"2022-05-22\"], \\\n",
    "                7:[\"Monaco Grand Prix\", \"2022-05-29\"], 8:[\"Azerbaijan Grand Prix\", \"2022-06-12\"], 9:[\"Canadian Grand Prix\",  \"2022-06-19\"], \\\n",
    "                10:[\"British Grand Prix\", \"2022-07-03\"], 11:[\"Austrian Grand Prix\", \"2022-07-10\"], 12:[\"French Grand Prix\", \"2022-07-24\"], \\\n",
    "                13:[\"Hungarian Grand Prix\", \"2022-07-31\"], 14:[\"Belgian Grand Prix\", \"2022-08-28\"], 15:[\"Dutch Grand Prix\", \"2022-09-04\"], \\\n",
    "                16:[\"Italian Grand Prix\", \"2022-09-11\"], 17:[\"Singapore Grand Prix\", \"2022-10-02\"], 18:[\"Japanese Grand Prix\", \"2022-10-09\"], \\\n",
    "                19:[\"United States Grand Prix\", \"2022-10-23\"], 20:[\"Mexico City Grand Prix\", \"2022-10-30\"], 21:[\"São Paulo Grand Prix\", \"2022-11-13\"]}\n",
    "dfList = []\n",
    "for key in raceCalendar:\n",
    "    \n",
    "    print(f\"Collecting Data for {raceCalendar[key][0]}\")\n",
    "    #Imola, Austria, and Brazil are sprint weekends, so they have a special case where sprint = true\n",
    "    if key not in [4,11,21]:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1]))\n",
    "    else:\n",
    "        dfList.append(weekendCompiler(raceCalendar[key][0], raceCalendar[key][1], sprint = True))\n",
    "\n",
    "masterList = pd.concat(dfList)\n",
    "masterList >>= drop(X.Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c937f",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb0571",
   "metadata": {},
   "source": [
    "Clean Data For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63740b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn_copy = masterList.copy()\n",
    "nn_copy = pd.get_dummies(nn_copy, columns = ['Weekend', 'Driver', 'Compound', 'QualifyingCompound'])\n",
    "\n",
    "#Adjust Boolean Data\n",
    "nn_copy['New'] = pd.to_numeric(nn_copy['New'], errors = 'coerce')\n",
    "\n",
    "nn_copy = nn_copy.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52f51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training variables(X) and prediction variables (Y)\n",
    "x = nn_copy >> drop(X.QualifyingLapTime)\n",
    "             \n",
    "y = nn_copy['QualifyingLapTime']\n",
    "\n",
    "#Normalize Training Data\n",
    "\n",
    "x_scaler = preprocessing.StandardScaler().fit(x)\n",
    "x_norm = x_scaler.transform(x)\n",
    "\n",
    "#Split Data Into Test and Training Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size = 0.2)\n",
    "\n",
    "x_train = np.asarray(x_train).astype(float)\n",
    "y_train = np.asarray(y_train).astype(float)\n",
    "x_test = np.asarray(x_test).astype(float)\n",
    "y_test = np.asarray(y_test).astype(float)\n",
    "\n",
    "#Split Test Data Into Validation and Test Sets\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c1bca",
   "metadata": {},
   "source": [
    "\"Standard\" Neural Network Following General Guidelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3188566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "347/347 - 1s - loss: 7455.8296 - mean_squared_error: 7455.8296 - mean_absolute_error: 85.5617 - 1s/epoch - 4ms/step\n",
      "Epoch 2/100\n",
      "347/347 - 1s - loss: 7191.5283 - mean_squared_error: 7191.5283 - mean_absolute_error: 83.9899 - 621ms/epoch - 2ms/step\n",
      "Epoch 3/100\n",
      "347/347 - 1s - loss: 6916.8657 - mean_squared_error: 6916.8657 - mean_absolute_error: 82.2843 - 621ms/epoch - 2ms/step\n",
      "Epoch 4/100\n",
      "347/347 - 1s - loss: 6653.6943 - mean_squared_error: 6653.6943 - mean_absolute_error: 80.5694 - 624ms/epoch - 2ms/step\n",
      "Epoch 5/100\n",
      "347/347 - 1s - loss: 6406.6548 - mean_squared_error: 6406.6548 - mean_absolute_error: 78.8777 - 605ms/epoch - 2ms/step\n",
      "Epoch 6/100\n",
      "347/347 - 1s - loss: 6175.5586 - mean_squared_error: 6175.5586 - mean_absolute_error: 77.2164 - 627ms/epoch - 2ms/step\n",
      "Epoch 7/100\n",
      "347/347 - 1s - loss: 5959.5942 - mean_squared_error: 5959.5942 - mean_absolute_error: 75.5836 - 605ms/epoch - 2ms/step\n",
      "Epoch 8/100\n",
      "347/347 - 1s - loss: 5756.8579 - mean_squared_error: 5756.8579 - mean_absolute_error: 73.9735 - 603ms/epoch - 2ms/step\n",
      "Epoch 9/100\n",
      "347/347 - 1s - loss: 5566.7456 - mean_squared_error: 5566.7456 - mean_absolute_error: 72.3877 - 612ms/epoch - 2ms/step\n",
      "Epoch 10/100\n",
      "347/347 - 1s - loss: 5389.2256 - mean_squared_error: 5389.2256 - mean_absolute_error: 70.8277 - 606ms/epoch - 2ms/step\n",
      "Epoch 11/100\n",
      "347/347 - 1s - loss: 5223.6460 - mean_squared_error: 5223.6460 - mean_absolute_error: 69.2962 - 613ms/epoch - 2ms/step\n",
      "Epoch 12/100\n",
      "347/347 - 1s - loss: 5069.5806 - mean_squared_error: 5069.5806 - mean_absolute_error: 67.7961 - 607ms/epoch - 2ms/step\n",
      "Epoch 13/100\n",
      "347/347 - 1s - loss: 4926.2988 - mean_squared_error: 4926.2988 - mean_absolute_error: 66.3303 - 606ms/epoch - 2ms/step\n",
      "Epoch 14/100\n",
      "347/347 - 1s - loss: 4793.0195 - mean_squared_error: 4793.0195 - mean_absolute_error: 64.9075 - 609ms/epoch - 2ms/step\n",
      "Epoch 15/100\n",
      "347/347 - 1s - loss: 4669.1558 - mean_squared_error: 4669.1558 - mean_absolute_error: 63.5257 - 637ms/epoch - 2ms/step\n",
      "Epoch 16/100\n",
      "347/347 - 1s - loss: 4554.1323 - mean_squared_error: 4554.1323 - mean_absolute_error: 62.1819 - 521ms/epoch - 2ms/step\n",
      "Epoch 17/100\n",
      "347/347 - 1s - loss: 4447.4644 - mean_squared_error: 4447.4644 - mean_absolute_error: 60.8719 - 660ms/epoch - 2ms/step\n",
      "Epoch 18/100\n",
      "347/347 - 1s - loss: 4348.5322 - mean_squared_error: 4348.5322 - mean_absolute_error: 59.5993 - 537ms/epoch - 2ms/step\n",
      "Epoch 19/100\n",
      "347/347 - 1s - loss: 4257.0054 - mean_squared_error: 4257.0054 - mean_absolute_error: 58.3691 - 686ms/epoch - 2ms/step\n",
      "Epoch 20/100\n",
      "347/347 - 1s - loss: 4172.2280 - mean_squared_error: 4172.2280 - mean_absolute_error: 57.1814 - 682ms/epoch - 2ms/step\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMSE, metrics \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError(),\n\u001b[0;32m     10\u001b[0m                                                                          tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanAbsoluteError()])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Start Batch Size Small (Powers of 2 for efficient GPU usage)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Epochs rule of thumb is to start with triple the amount of columns\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Verbose determines how it prints the training progress\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Evaluate Model \u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(x_validation, y_validation, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#One layer is generally enough for simple problems like this (As opposed to something like digit recognition)\n",
    "#Hidden layer size is generally between the input size (number of x variables) and the output size (number of y variables)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "\n",
    "#Adam is the recommended default optimizer\n",
    "#Mean squared error loss function as seen in class\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "#Start Batch Size Small (Powers of 2 for efficient GPU usage)\n",
    "#Epochs rule of thumb is to start with triple the amount of columns\n",
    "#Verbose determines how it prints the training progress\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)\n",
    "\n",
    "#Evaluate Model \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd464f54",
   "metadata": {},
   "source": [
    "What If We Adjust The Network Architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f33a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is good\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e230a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is best\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate best model with test data\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7496c07",
   "metadata": {},
   "source": [
    "What if we change the activation function? (They are all worse than relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'linear'),\n",
    "    tf.keras.layers.Dense(16, activation = 'linear'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99571dcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(16, activation = 'sigmoid'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(16, activation = 'tanh'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 2)      \n",
    "model.evaluate(x_validation, y_validation, verbose = 2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7dcbf",
   "metadata": {},
   "source": [
    "What if we adjust batch size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74418d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is bad\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 64, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d007e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is superior\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 8, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is even more superior (Not going smaller because run time is very long)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.MSE, metrics = [tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                         tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size = 4, epochs = 100, verbose = 0)\n",
    "model.evaluate(x_validation, y_validation, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52939d",
   "metadata": {},
   "source": [
    "What if we add more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Much better but takes a long time to train and could cause overfitting (Best model with all the data)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                   tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1000, verbose=2)\n",
    "model.evaluate(x_validation, y_validation, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce465a13",
   "metadata": {},
   "source": [
    "Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837079e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39386aa7",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genModel(layers, batchSize=32, epochs=100, verbose=2):\n",
    "    \"\"\"\n",
    "    Automated neural network generator\n",
    "    Input: list of layers, batch size, number of epochs, verbosity\n",
    "    Output: it do the thing\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential(layers)\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                                                                   tf.keras.metrics.MeanAbsoluteError()])\n",
    "    model.fit(x_train, y_train, batchSize, epochs, verbose)\n",
    "    model.evaluate(x_validation, y_validation, verbose)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "class PlotData():\n",
    "    \"\"\"\n",
    "    A module for storing plot data in a more portable, reusable form\n",
    "    Stores x and y data and metadata (title, labels, etc.)\n",
    "    Parameters: 2 arrays of x and y datasets (to allow multiple relations per plot), \n",
    "                plot type as a string, title, axis labels, and grid boolean\n",
    "    \"\"\"\n",
    "    def __init__(self, xdata, ydata, ptype='plot', title='Figure', xlabel='x axis', ylabel='y axis', grid=True):\n",
    "        \n",
    "        # Only check these if we have ydata\n",
    "        if ydata != None:\n",
    "            # Throws error if xdata and ydata length do not match\n",
    "            if not len(xdata) == len(ydata):\n",
    "                raise Exception('Data mismatch error: must have the same number of corresponding x and y fields')\n",
    "            \n",
    "            # Throws error if x and y subdata length do not match\n",
    "                for i in range(len(xdata)):\n",
    "                    if not len(xdata[i]) == len(ydata[i]):\n",
    "                        raise Exception(f'Data mismatch error: {i}th set of x and y data have unequal length')\n",
    "        \n",
    "            \n",
    "        self.xdata = xdata\n",
    "        self.ydata = ydata\n",
    "        self.ptype = ptype\n",
    "        self.title = title\n",
    "        self.xlabel = xlabel\n",
    "        self.ylabel = ylabel\n",
    "        self.grid = grid\n",
    "        \n",
    "        \n",
    "    def plot(self, style='b--'):\n",
    "        \"\"\"\n",
    "        May not be showing it but it do be plotting the data\n",
    "        Input: Optional style\n",
    "        \"\"\"\n",
    "        # Plots based on type\n",
    "        if self.ptype == 'plot':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.plot(self.xdata[i], self.ydata[i], style)\n",
    "                \n",
    "        elif self.ptype == 'scatter':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.scatter(self.xdata[i], self.ydata[i], style)\n",
    "            \n",
    "        elif self.ptype == 'bar':\n",
    "            for i in range(len(self.xdata)):\n",
    "                plt.bar(self.xdata[i], self.ydata[i])\n",
    "                \n",
    "        elif self.ptype == 'boxplot':\n",
    "            plt.boxplot(self.xdata)\n",
    "            \n",
    "        else:\n",
    "            print(f'{self.title} has invalid plot type')\n",
    "                \n",
    "        plt.title(self.title)\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        if self.grid:\n",
    "            plt.grid()\n",
    "            \n",
    "            \n",
    "    def addData(x, y):\n",
    "        \"\"\"\n",
    "        Clears the figure and adds a new x-y relation to the data\n",
    "        Input: x and y data to add\n",
    "        Output: It adds the data wow\n",
    "        \"\"\"\n",
    "        plt.clf()\n",
    "        self.xdata.append(x)\n",
    "        self.ydata.append(y)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35125264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter():\n",
    "    \"\"\"\n",
    "    Takes plot data and auto-generates subplots\n",
    "    Parameters: a list of PlotData objects\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, plots, subdim=None):\n",
    "        self.plots = plots\n",
    "        self.subdim = subdim\n",
    "        self.arrange(self.subdim)\n",
    "        self.figsize = (5*self.subdim[1], 5*self.subdim[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def addPlot(plot, loc=-1, subdim=None):\n",
    "        \"\"\"\n",
    "        Adds a new subplot and rearranges the figure\n",
    "        Input: the PlotData to add, the location to display it, and a custom aspect ratio if desired\n",
    "        Output:  It does it\n",
    "        \"\"\"\n",
    "        self.plots.insert(loc, plot)\n",
    "        if not subdim == None:\n",
    "            self.subdim = subdim\n",
    "        else:\n",
    "            arrange()\n",
    "    \n",
    "    \n",
    "    def addData(x, y, loc):\n",
    "        \"\"\"\n",
    "        Adds data to a subplot\n",
    "        Input: x and y data to add, and which subplot to add to\n",
    "        Output: It also does it\n",
    "        \"\"\"\n",
    "        self.plots[loc].addData(x, y)\n",
    "    \n",
    "    \n",
    "    def arrange(self, subdim=None):\n",
    "        \"\"\"\n",
    "        Helper function to redistrubte subplots to maximize squareness or fulfill user-set dimensions\n",
    "        Input: Optional custom dimensions\n",
    "        Output: :)\n",
    "        \"\"\"\n",
    "        if not subdim == None:\n",
    "            # Throw error if there are not enough subplot spaces\n",
    "            if subdim[0] * subdim[1] < len(self.plots):\n",
    "                raise Exception('Specified plot dimensions cannot fit specidied subplot data. Try doing math')\n",
    "            self.subdim = subdim\n",
    "        else:\n",
    "            # Find the arrangement closest to square that fills all subplots\n",
    "            for i in range(int(np.sqrt(len(self.plots))), 0, -1):\n",
    "                if len(self.plots) % i == 0:\n",
    "                    self.subdim = (i, len(self.plots) // i)\n",
    "                    break\n",
    "        \n",
    "                \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Generates the figure\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        for i in range(len(self.plots)):\n",
    "            plt.subplot(self.subdim[0], self.subdim[1], i + 1)\n",
    "            self.plots[i].plot()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDrivers(weekendDF):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of driver IDs to driver stats\n",
    "    Input: Dataframe\n",
    "    Output: Dictionary of driver IDs to driver stat dataframe (which is a subset of the input)\n",
    "    \"\"\"\n",
    "    idList = list(set(weekendDF['Driver'].values))\n",
    "    return {ID : weekendDF.loc[weekendDF['Driver'] == ID] for ID in idList}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539912fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverages(dataframe, race, driverIDs, category):\n",
    "    \"\"\"\n",
    "    Generates a list of average values for a driver stat in a given race\n",
    "    Input: Dictionary of races to dictionaries of IDs to driver dataframes, the race to \n",
    "           select, the driver to select, and the driver stat to average\n",
    "    Output: The mean value\n",
    "    \"\"\"\n",
    "    \n",
    "    averages = []\n",
    "    for ID in driverIDs:\n",
    "        # If the driver is not present they get 0\n",
    "        try:\n",
    "            averages.append(dataframe[race][ID][category].values.mean().item() / 1.0e9)\n",
    "        except:\n",
    "            averages.append(0)\n",
    "    return averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim down to just the data we're interested in\n",
    "categories = ['Driver', 'LapTime', 'Sector1Time', 'Sector2Time','Sector3Time', 'Compound', 'QualifyingLapTime', \n",
    "          'QualifyingSector1Time', 'QualifyingSector2Time','QualifyingSector3Time', 'QualifyingCompound', 'Weekend']\n",
    "plotDataFrame = masterList.copy()\n",
    "\n",
    "# Split data into a dictionary by race\n",
    "raceNames = [race[0] for race in raceCalendar.values()]\n",
    "raceDataFrames = {race : plotDataFrame.loc[plotDataFrame[\"Weekend\"] == race] for race in raceNames}\n",
    "\n",
    "# Split dictionary of races further into to dictionaries of driver IDs to driver data\n",
    "# Get a specific stat with driverDataFrames['raceName']['driverID']['field'].values\n",
    "driverDataFrames = {name : genDrivers(raceDataFrames[name]) for name in raceNames}\n",
    "\n",
    "# Make sorted list of driver IDs\n",
    "driverIDs = list(set([int(ID) for ID in plotDF['Driver'].values]))\n",
    "driverIDs.sort()\n",
    "driverIDs = [str(ID) for ID in driverIDs]\n",
    "\n",
    "# Select races to plot\n",
    "selectedRaces = raceNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77984506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataField(field):\n",
    "    \"\"\"\n",
    "    Takes a field name and produces a dictionary of races to dictionaries of drivers to numpy arrays of driver data\n",
    "    \"\"\"\n",
    "    dataField = {}\n",
    "    buffer = {}\n",
    "    \n",
    "    for race in selectedRaces:\n",
    "        for ID in driverIDs:\n",
    "            try:\n",
    "                arr = driverDataFrames[race][ID][field].values\n",
    "                if arr.dtype == 'timedelta64[ns]':\n",
    "                    buffer[ID] = arr.astype('float64') / 1e9\n",
    "                else:\n",
    "                    buffer[ID] = arr\n",
    "            except:\n",
    "                buffer[ID] = np.array([0])\n",
    "                \n",
    "        dataField[race] = buffer.copy()\n",
    "        buffer.clear()\n",
    "        \n",
    "    return dataField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotField(field, saveName=None):\n",
    "    \"\"\"\n",
    "    Given a field name, generates boxplots for that field for each driver across each race, optionally saves an image\n",
    "    \"\"\"\n",
    "    lapTimes = getDataField(field)\n",
    "    fieldPlotData = []\n",
    "\n",
    "    for race, driverData in lapTimes.items():\n",
    "        raceData = [driverField for driverField in driverData.values()]\n",
    "        title = f'{field} of drivers in {race}'\n",
    "        fieldPlotData.append(PlotData(raceData, None, 'boxplot', title, 'Drivers', field))\n",
    "        \n",
    "    plotter = Plotter(fieldPlotData)\n",
    "    plotter.plot()\n",
    "    \n",
    "    if saveName != None:\n",
    "        plt.savefig(saveName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daec051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots and saves each dataframe field\n",
    "# FIXME: plots for some races are mostly blank or show negative values?\n",
    "timeFields = ['LapTime', 'Sector1Time', 'Sector2Time','Sector3Time']\n",
    "speedFields = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']\n",
    "environmentFields = ['AirTemp', 'Humidity', 'Pressure', 'TrackTemp', 'Windspeed']\n",
    "\n",
    "qtimeFields = ['QualifyingLapTime', 'QualifyingSector1Time', 'QualifyingSector2Time','QualifyingSector3Time']\n",
    "qspeedFields = ['QualifyingSpeedI1', 'QualifyingSpeedI2', 'QualifyingSpeedFL', 'QualifyingSpeedST']\n",
    "\n",
    "for field in speedFields:\n",
    "    plotField(field, field + 'BoxPlots.png')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
